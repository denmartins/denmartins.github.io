{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60aa925",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Modelo GPT - Parte 1\n",
    "\n",
    "## [Tópicos em Ciência de Dados](https://denmartins.github.io/teaching/2025-topicos-cd)\n",
    "\n",
    "### [Prof. Dr. Denis Mayr Lima Martins](https://denmartins.github.io/)\n",
    "\n",
    "### [Pontifícia Universidade Católica de Campinas](https://www.puc-campinas.edu.br/)\n",
    "\n",
    "<img src=\"https://www.puc-campinas.edu.br/wp-content/uploads/2022/06/logo-puc.png\" width=\"100px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8405f1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Objetivos de Aprendizagem\n",
    "\n",
    "- Implementar um modelo de LLM semelhante ao GPT que pode ser treinado para gerar texto.\n",
    "- Compreender o conceito de normalização de camadas e sua importância no treinamento de redes neurais.\n",
    "<!-- - Entender como conexões de atalho (skipping connections) em redes neurais profundas ajudam no treinamento.\n",
    "- Implementar blocos Transformer para criar modelos GPT de diferentes tamanhos -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e84d4-8bd3-4620-8a55-f86b58c4e567",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "Baseado no Livro <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> de <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"200px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b24995",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implementando um Modelo GPT (Parte 1)\n",
    "\n",
    "Com o mecanismo de atenção disponível, agora iremos programar os outros blocos de construção de um LLM e montá-los em um modelo do tipo GPT que pode ser treinado para gerar texto convincente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2f8142",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd8dad",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- `import os`: Importa o módulo *os* para interagir com o sistema operacional.  \n",
    "- `os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"`: Define uma variável de ambiente que instrui a biblioteca MKL da Intel a permitir que múltiplas instâncias da mesma biblioteca sejam carregadas simultaneamente, sem gerar erro. Tal configuração costuma ser empregada para resolver conflitos entre bibliotecas em tarefas de computação científica ou aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7331b1e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.5\n",
      "torch version: 2.8.0\n",
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2accc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. Codificando uma Arquitetura de LLM\n",
    "\n",
    "- Modelos de linguagem de larga escala (LLMs), como o GPT, são arquiteturas de redes neurais profundas projetadas para gerar novo texto palavra (ou token) por palavra. Apesar do tamanho, a arquitetura do modelo é menos complicada do que se imagina, já que muitos de seus componentes são repetidos.\n",
    "- A arquitetura de um GPT contém, ao lado das camadas de embedding, blocos *transformer* que incluem o módulo de atenção multi-cabeças mascarada implementado anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765936a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length, which might be much larger in other models\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers/transformer blocks\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8f265",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- **`\"vocab_size\"`** tamanho de vocabulário de 50257 palavras, suportado pelo tokenizador BPE.  \n",
    "- **`\"context_length\"`** contagem máxima de tokens de entrada do modelo, conforme habilitado pelos embeddings posicionais.  \n",
    "- **`\"emb_dim\"`** tamanho do embedding para os tokens de entrada, convertendo cada token de entrada num vetor de 768 dimensões.  \n",
    "- **`\"n_heads\"`** número de cabeças de atenção no mecanismo de multi‑head attention.  \n",
    "- **`\"n_layers\"`** número de blocos transformer dentro do modelo, que iremos implementar em breve.  \n",
    "- **`\"drop_rate\"`** descarta 10% das unidades ocultas durante o treinamento para mitigar overfitting.  \n",
    "- **`\"qkv_bias\"`** decide se as camadas `Linear` no mecanismo de MHA devem incluir um vetor de bias ao calcular os tensores query (Q), key (K) e value (V); desativaremos essa opção, prática padrão em LLMs modernos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dabe6-ff55-4479-9909-6bbfa0a8874f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implementação Principal\n",
    "\n",
    "Arquitetura inicial chamada que serve como esqueleto do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cd78d-4764-4bb1-84fe-885a5cacffea",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # a camada de entrada mapeia do tamanho do vocabulário para o espaço de embedding\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        # Use um placeholder para TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(*[SimpleTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        # Use um placeholder para LayerNorm\n",
    "        self.final_norm = SimpleLayerNorm(cfg[\"emb_dim\"])\n",
    "        # camada de saída, mapeia do espaço de embedding de volta ao tamanho do vocabulário\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42095c-e987-47ad-bda3-4854068d8d3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Camadas adicionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19db1f5-1be2-454e-87db-bd6a116e6ad5",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Um placeholder simples\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Este bloco não faz nada e apenas retorna sua entrada.\n",
    "        return x\n",
    "\n",
    "class SimpleLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # Os parâmetros aqui são apenas para imitar a interface do LayerNorm.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Esta camada não faz nada e apenas retorna sua entrada.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee0e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "A classe `SimpleGPTModel` presente neste código define uma versão simplificada de um modelo do tipo GPT utilizando o módulo de redes neurais do PyTorch (`nn.Module`). A arquitetura do modelo na classe `SimpleGPTModel` consiste em embeddings de token e posicional, dropout, uma série de blocos transformadores (`SimpleTransformerBlock`), normalização de camada final (`SimpleLayerNorm`) e uma camada linear de saída (out_head). A configuração é passada por meio de um dicionário Python; por exemplo, o dicionário `GPT_CONFIG_124M` que criamos anteriormente.  \n",
    "\n",
    "O método `forward` descreve o fluxo de dados através do modelo: ele calcula os embeddings de token e posicional para os índices de entrada, aplica dropout, processa os dados pelos blocos transformadores, aplica a normalização e, finalmente, gera logits com a camada linear de saída.  \n",
    "\n",
    "O código já é funcional, como veremos mais adiante após prepararmos os dados de entrada. Entretanto, atualmente utilizamos placeholders (`SimpleLayerNorm` e `SimpleTransformerBlock`) para o bloco transformador e a normalização de camada.  \n",
    "\n",
    "Em seguida, prepararemos os dados de entrada e inicializaremos um novo modelo GPT para ilustrar seu uso. Construindo sobre as figuras que vimos no episódio em que codificamos o tokenizador, a figura abaixo fornece uma visão geral de alto nível de como os dados entram e saem de um modelo GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd45db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Visão Geral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead1bfb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/e065aaca707d5039aea24d8752be69ab78a7b19028724cfacf0902d8b46ae9ed/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f30342e776562703f313233\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c048b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tokenização\n",
    "\n",
    "A saída do código abaixo é o que a LLM recebe, e a tarefa consiste em produzir a próxima palavra desse texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead01173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"  # every word will result in a token\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa92f55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Instanciando o Modelo\n",
    "\n",
    "Inicializamos uma nova instância do `SimpleGPTModel` com 124 milhões de parâmetros conforme especificado acima e alimentamos o modelo com o lote tokenizado (os resultados do modelo são comumente denominados *logits*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d695feb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = SimpleGPTModel(GPT_CONFIG_124M)  \n",
    "logits = model(batch)                   \n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37711e07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "O tensor de saída possui duas linhas correspondentes às duas amostras de texto. Cada amostra consiste em 4 tokens (um para cada palavra); cada token é um vetor de 50257 dimensões, o que corresponde ao tamanho do vocabulário do tokenizador.\n",
    "\n",
    "Embeddings tem 50257 dimensões porque cada uma dessas dimensões representa um token único no vocabulário. Ao final deste episódio, quando implementarmos o código de pós‑processamento, converteremos esses vetores de 50257 dimensões de volta em IDs de token, que então poderemos decodificar em palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276c444",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2. Layer Normalization\n",
    "\n",
    "\n",
    "Usamos a normalização de camada para melhorar a estabilidade e eficiência do treinamento de redes neurais. \n",
    "- **Ideia central**: ajustar as ativações (saídas) de uma camada de rede neural de modo que tenham média zero e variância unitária, também conhecida como *unit variance*. \n",
    "- **Vantagem**: Acelera a convergência para pesos efetivos e garante um treinamento consistente e confiável. \n",
    "- Nas arquiteturas GPT‑2 e nos transformers, a normalização de camada costuma ser aplicada antes e depois do módulo de atenção multi‑cabeça e antes da camada de saída final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65d70c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](https://camo.githubusercontent.com/1bb0018e68d16529b969ccc6b0bca371ed5a7b9a514d1bd3f345c4941c0f463d/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f30352e77656270)\n",
    "\n",
    "Vamos observar como funciona a normalização de camada passando uma pequena amostra de entrada por uma camada neural simples; especificamente, recriamos o exemplo ilustrado na figura acima através do código seguinte, no qual implementamos uma camada neural com 5 entradas e 6 saídas que aplicaremos a dois exemplos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66ea375",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Cria 2 exemplos de treino com 5 features cada\n",
    "batch_example = torch.randn(2, 5) \n",
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a4b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d889f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# -1 torna invariante contra dimensões adicionais\n",
    "mean = out.mean(dim=-1, keepdim=True) \n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46683ea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Ilustrando o cálculo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c49af9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![](https://camo.githubusercontent.com/1d353a009cf6ec3a2ea918b62bfedc10b565e0d17836366fc0e5dffca6156715/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f30362e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38dd42a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Aplica a normalização de camada aos resultados da camada anterior\n",
    "# A operação consiste em subtrair a média e dividir pelo desvio padrão.\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "# melhora a visualização\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8102df1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Layer Norm (módulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dda8d6a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):  # valores na dimensão do embedding\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        # isso torna os valores treináveis\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))  \n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)  # eps evita divisão por zero\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "        # shift não tem efeito aqui pois adiciona apenas zero; será relevante mais tarde durante o treinamento,\n",
    "        # permitindo que a rede desfaça essa normalização; similarmente para scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e633e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Layer Norm (aplicação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cc3eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
       "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "out_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfcadba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c359ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Observações\n",
    "\n",
    "- **Escala e deslocamento:** além da normalização (subtração da média e divisão pela variância), são introduzidos dois parâmetros treináveis, `scale` e `shift`. Inicialmente eles não alteram o valor (multiplicação por 1 e adição de 0), mas podem ser ajustados durante o treinamento para otimizar a performance do modelo.  \n",
    "- **Variância enviesada:** ao calcular a variância é usado `unbiased=False`, ou seja, a fórmula $\\frac{\\sum_i(x_i-\\bar{x})^2}{n}$. Essa escolha produz uma estimativa enviesada, mas em dimensões de embedding muito grandes a diferença entre $n$ e $n-1$ é insignificante. O GPT‑2 foi treinado com essa configuração, portanto o mesmo padrão é adotado para garantir compatibilidade com pesos pré‑treinados.  \n",
    "O texto conclui convidando à experimentação prática do `LayerNorm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2c32b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Resumo\n",
    "\n",
    "- **Arquitetura GPT**  \n",
    "  - Bloco transformer auto‑regressivo (self‑attention + MLP).  \n",
    "  - Pré‑treinado em previsão de próximo token; fine‑tune para tarefas específicas.  \n",
    "  - Escalável: mais camadas, cabeças e dimensões → GPT‑2/3/4.\n",
    "- **Normalização de Camada**  \n",
    "  - Estabiliza ativações sem depender do batch (útil em sequências variáveis).  \n",
    "  - Colocada após cada sub‑camada e antes da soma residual.  \n",
    "  - Melhora convergência, evita saturação de ativação e mantém gradientes adequados.\n",
    "- **Impacto**  \n",
    "  - Modelos GPT com LayerNorm treinam mais rápido, são mais robustos e mantêm desempenho consistente em geração de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28ad29-02a3-48cb-b709-2ff20f9dcb79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Próximos Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5831f9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![](https://camo.githubusercontent.com/8ff24e3fedda0855855eacb08b6b1f78eac62471cef72b19539c068ec1c3bcda/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f30372e77656270)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": "80%",
   "overlay": "",
   "reveal_shortcuts": {
    "chalkboard": {
     "clear": "ctrl-k"
    },
    "main": {
     "toggleOverview": "tab"
    }
   },
   "scroll": true,
   "slideNumber": "c/t",
   "theme": "white",
   "transition": "slide",
   "width": "80%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
