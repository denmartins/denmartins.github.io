{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60aa925",
   "metadata": {
    "editable": true,
    "id": "e60aa925",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Instruction Finetuning\n",
    "\n",
    "## [Tópicos em Ciência de Dados](https://denmartins.github.io/teaching/2025-topicos-cd)\n",
    "\n",
    "### [Prof. Dr. Denis Mayr Lima Martins](https://denmartins.github.io/)\n",
    "\n",
    "### [Pontifícia Universidade Católica de Campinas](https://www.puc-campinas.edu.br/)\n",
    "\n",
    "<img src=\"https://www.puc-campinas.edu.br/wp-content/uploads/2022/06/logo-puc.png\" width=\"100px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8405f1b",
   "metadata": {
    "editable": true,
    "id": "d8405f1b",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objetivos de Aprendizagem\n",
    "---\n",
    "\n",
    "*   Explicar a necessidade e função do Ajuste Fino de Instruções (AFI)\n",
    "*   Descrever a metodologia de treinamento e perda: detalhar a estrutura dos dados de AFI (instrução, contexto, resposta alvo) e explicar a função da perda\n",
    "*   Avaliar o desempenho do modelo: utilizar métricas de avaliação e entender o papel de modelos externos (*LLM-as-a-Judge*) na avaliação da qualidade do alinhamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e84d4-8bd3-4620-8a55-f86b58c4e567",
   "metadata": {
    "editable": true,
    "id": "b64e84d4-8bd3-4620-8a55-f86b58c4e567",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "Baseado no Livro <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> de <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"200px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a0f1a1",
   "metadata": {
    "editable": true,
    "id": "17a0f1a1",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Relembrando: O Conceito de Fine-Tuning\n",
    "---\n",
    "\n",
    "*   **Definição:** É o processo de utilizar um modelo pré-treinado como base e treiná-lo adicionalmente em um *dataset* menor e específico de um domínio ou tarefa.\n",
    "*   **Objetivo:** Adaptar o modelo ao novo contexto, aprimorando o desempenho em aplicações especializadas, como tradução de linguagem, análise de sentimento ou sumarização.\n",
    "*   **Vantagem:** O *fine-tuning* se baseia no conhecimento pré-existente do modelo, o que reduz substancialmente os requisitos computacionais e de dados em comparação com o treinamento do modelo do zero (*pre-training*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301aff55",
   "metadata": {
    "editable": true,
    "id": "301aff55",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Por Que os LLMs Tradicionais Falham com Diretivas\n",
    "---\n",
    "\n",
    "*   **Objetivo do Pré-treinamento:** Os LLMs são otimizados para o reconhecimento de padrões linguísticos, minimizando o erro de previsão contextual da próxima palavra em vastos *corpora*.\n",
    "    *   O modelo prevê o próximo *token* em uma sequência com base em padrões estatísticos.\n",
    "*   **A Limitação:** Este objetivo de previsão do próximo *token* não otimiza inerentemente o modelo para seguir instruções explícitas do usuário.\n",
    "    *   Sem treinamento adicional, um LLM de base simplesmente *completa* um *prompt*, em vez de fornecer uma *resposta* útil.\n",
    "    *   Exemplo: Solicitar \"*me ensine a fazer pão*\" pode resultar em \"*em um forno de casa*\" (uma conclusão gramaticalmente correta, mas inútil).\n",
    "*   **A Solução:** O AFI refina o modelo pré-treinado para interpretar as consultas do usuário como instruções formais que exigem ações específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cf96d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## O Que é Ajuste Fino de Instruções (AFI)?\n",
    "---\n",
    "\n",
    "*   **Definição:** AFI é uma técnica de ajuste fino que refina LLMs pré-treinados para aderir a instruções de tarefas específicas.\n",
    "*   **Metodologia:** Envolve treinamento supervisionado em conjuntos de dados que consistem em pares explícitos de *prompt*-resposta.\n",
    "*   **Função Chave:** O AFI preenche a lacuna entre a capacidade inerente de previsão da próxima palavra do LLM e o objetivo definido pelo ser humano de aderir a diretivas.\n",
    "*   **Benefícios:**\n",
    "    1.  **Alinhamento:** Conecta o objetivo de pré-treinamento com o objetivo de seguir instruções.\n",
    "    2.  **Controlabilidade:** Restringe os *outputs* do modelo para alinhá-los com as características desejadas (e.g., formato ou conhecimento de domínio).\n",
    "    3.  **Generalização:** Modelos ajustados por instruções demonstram forte desempenho *zero-shot* e *few-shot* em tarefas não vistas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe31f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Anatomia de uma Amostra de Dados AFI\n",
    "---\n",
    "\n",
    "O AFI requer pares de instruções e seus *outputs* de alta qualidade correspondentes.\n",
    "\n",
    "*   **1. Instrução (A Diretiva):** Define claramente a tarefa necessária.\n",
    "    *   *Exemplo:* \"Traduza a seguinte frase para o Francês\".\n",
    "*   **2. Input/Contexto (O Conteúdo):** Informações suplementares opcionais relevantes para a tarefa.\n",
    "    *   *Exemplo:* \"A frase a traduzir: 'O processo de ajuste fino é complexo.'\".\n",
    "*   **3. Resposta Alvo (A Resposta Ouro):** O *output* de referência de alta qualidade que demonstra a conclusão correta da tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51e246",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Style Template\n",
    "---\n",
    "\n",
    "<center>\n",
    "<img src=\"https://camo.githubusercontent.com/a0d3a2f932be145f5afd61d2abf54db358353ed2d30b4f5a54c508ac01a65b74/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f30342e776562703f32\" width=\"700px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc8579",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Estratégias de Coleta de Dados I\n",
    "---\n",
    "\n",
    "A curadoria e o dimensionamento de pares de instrução-*output* de alta qualidade são desafios centrais do AFI.\n",
    "\n",
    "*   **1. Dados Criados por Humanos:** Dados anotados manualmente ou obtidos diretamente, confiando apenas na coleta e verificação humana.\n",
    "    *   **Prós:** Geralmente a mais alta qualidade e consistência.\n",
    "    *   **Contras:** Demorado e custoso para grandes escalas.\n",
    "    *   Exemplos: [Databricks Dolly](https://github.com/databrickslabs/dolly/tree/master/data) (15K instâncias, abrangendo 7 tipos como Q&A, escrita criativa), [Meta LIMA](https://arxiv.org/pdf/2305.11206) (1K [exemplos](https://huggingface.co/datasets/GAIR/lima) cuidadosamente selecionados).\n",
    "*   **2. Integração de Dados de Conjuntos de Dados Anotados:**\n",
    "    *   Envolve a conversão de conjuntos de dados de PLN existentes (e.g., NLI, análise de sentimentos) em pares de instrução-*output* usando *templates*.\n",
    "    *   Isto formaliza diversas tarefas de PLN em um formato unificado *sequence-to-sequence*.\n",
    "    *   Exemplos: [FLAN](https://github.com/google-research/FLAN/tree/main/flan/v2) (transforma 62 *benchmarks* de PLN), [P3](https://huggingface.co/datasets/bigscience/P3) (integra 170 conjuntos de dados de PLN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20185b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Estratégias de Coleta de Dados II\n",
    "---\n",
    "\n",
    "LLMs podem ser usados para aumentar conjuntos de dados de AFI quando a criação manual é inviável.\n",
    " \n",
    "*   [**Self-Instruct**](https://arxiv.org/pdf/2212.10560): Começa com um pequeno conjunto de pares sementes. Um LLM gera novas instruções e outra instância gera *outputs* correspondentes.\n",
    "    *   *Exemplo:* O modelo [Alpaca usou 52K pares sintéticos](crfm.stanford.edu/2023/03/13/alpaca.html) gerados desta forma.\n",
    "*   [**Bonito**](https://arxiv.org/pdf/2402.18334): Converte texto não anotado em datasets de treino para AFI. Modelo base: Mistral-7B\n",
    "*   [**Magpie**](https://arxiv.org/html/2406.08464v2): Gera dados de instrução solicitando a um LLM alinhado (e.g., Llama 3 8B Instruct) com um *template* de pré-consulta para sintetizar instruções e respostas de forma totalmente automática.\n",
    "<!-- *   **Autoaperfeiçoamento (Evol-Instruct):** Aumenta a complexidade da instrução.\n",
    "    *   **Evolução em Profundidade:** Solicita a um LLM injetar restrições (e.g., limites de palavras) ou aumentar os passos de raciocínio.\n",
    "    *   **Evolução em Amplitude:** Solicita ao LLM gerar instruções totalmente novas em domínios sub-representados. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d54ddf-cd85-4759-bdac-6141ad1352ee",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Alpaca\n",
    "---\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://api.wandb.ai/files/capecape/images/projects/38233410/f8eba1d5.png' alt='Visão Geral Alpaca' style=\"width:800px;\"/>\n",
    "    <span style='display:block;'>Workflow Alpaca. AFI sobre o modelo base Llama-7B. Qualidade similar ao modelo da OpenAI, mas muito menor e mais barato de reproduzir. Fonte: <a href=\"https://wandb.ai/capecape/alpaca_ft/reports/How-to-fine-tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2\" target=\"_blank\">Weights and Biases</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450f7fb-4aad-4ce9-b32e-d2d546e2f76d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bonito\n",
    "---\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://towardsdatascience.com/wp-content/uploads/2024/03/06Y_mH9Oik8938xgu.png' alt='Visão Geral Bonito' style=\"width:700px;\"/>\n",
    "    <span style='display:block;'>Workflow do framework Bonito. Fonte: <a href=\"https://arxiv.org/pdf/2402.18334\" target=\"_blank\">Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38992327-ca3f-4bad-8eb3-8c0cf2c8d811",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Magpie\n",
    "---\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://raw.githubusercontent.com/magpie-align/magpie/main/figs/overview.png' alt='Visão Geral Magpie' style=\"width:800px;\"/>\n",
    "    <span style='display:block;'>Workflow do framework Magpie. Step 1: apenas pre-query template como entrada para LLM e geração autorregressiva de instrução. Step 2: Combinação de post-query template e outra pre-query template. Fonte: <a href=\"https://arxiv.org/html/2406.08464v2\" target=\"_blank\">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c578b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pré-processamento\n",
    "---\n",
    "\n",
    "1.  **Formatação do Prompt:** Adote um estilo de *prompt* consistente (e.g., Alpaca) para todas as amostras de treinamento.\n",
    "2.  **Tokenização:** Converta o texto formatado de instrução-resposta em IDs de *token*.\n",
    "3.  **Colagem/Preenchimento Customizado (packing):** Uma função de colagem customizada é usada para preencher sequências dentro de um lote (ou *batch*) até o comprimento da sequência mais longa nesse lote.\n",
    "4.  **Mascaramento de Instrução (Opcional):** Mascarar IDs de *token* que correspondem à instrução impede que a função de perda seja calculada sobre o texto da instrução. Isto força o modelo a focar o treinamento na geração da *resposta*. Mas... (veja figura ao lado).\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src='https://substackcdn.com/image/fetch/$s_!mC_S!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5293f2b-4861-42d3-a921-bbd4fbc2afc1_1600x684.png' alt='Instruction Masking x Instruction Modeling' style=\"width:500px;\"/>\n",
    "<span style='display:block;'>Instruction Masking x Instruction Modeling (não mascara instrução) em (<a href=\"https://arxiv.org/html/2405.14394v2\" target=\"_blank\">https://arxiv.org/html/2405.14394v2</a>). Fonte: <a href=\"https://magazine.sebastianraschka.com/p/llm-research-insights-instruction\" target=\"_blank\">Raschka</a>.</span>\n",
    "<br/>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78df50-8da3-44c0-b53d-ff6509411829",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Packing\n",
    "---\n",
    "\n",
    "<center>\n",
    "<table style=\"width:100%;border:none;\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle;text-align:center;border:none;\">\n",
    "<img src='https://lweitkamp.github.io/posts/packing/packed_sequences.png' alt='Visão Geral Packing 1' style=\"width:800px;\"/>\n",
    "    <span style='display:block;'>Packing: Combinando múltiplas amostras em uma única sentença. Fonte: <a href=\"https://lweitkamp.github.io/posts/packing/index.html\" target=\"_blank\">Laurens Weitkamp</a>.</span>\n",
    "    <br/>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle;text-align:center;border:none;\">\n",
    "    <img src='https://api.wandb.ai/files/capecape/images/projects/38233410/d9f4c0c2.png' alt='Visão Geral Packing 2' style=\"width:800px;\"/>\n",
    "    <span style='display:block;'>Packing: Otimização do tamanho do contexto. Fonte: <a href=\"https://wandb.ai/capecape/alpaca_ft/reports/How-to-fine-tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2\" target=\"_blank\">Weights and Biases</a>.</span>\n",
    "    <br/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb71fb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Eficiência: Ajuste Fino com Eficiência de Parâmetros (PEFT)\n",
    "---\n",
    "\n",
    "O Ajuste Fino Completo (*Full Fine-Tuning*) é caro e corre o risco de *esquecimento catastrófico*. Os métodos PEFT reduzem drasticamente os custos.\n",
    "\n",
    "*   **Low-Rank Adaptation (LoRA):** A técnica PEFT mais comum.\n",
    "    *   Mantém a maioria dos parâmetros LLM pré-treinados congelados.\n",
    "    *   Injeta pequenas matrizes de decomposição de baixa classificação ($A$ e $B$) nos parâmetros de atenção.\n",
    "    *   Reduz drasticamente o número de parâmetros treináveis (e.g., 10.000x de redução para GPT-3) e o uso de memória.\n",
    "*   **Quantized LoRA (QLoRA):** Uma extensão do LoRA otimizando ainda mais a memória.\n",
    "    *   Quantiza os pesos base do LLM congelado para precisão ultrabaixa (e.g., 4-bit).\n",
    "    *   Permite o ajuste fino de modelos de alta qualidade usando uma única GPU de consumo.\n",
    "* Veja também o vídeo no Youtube: [LoRA explained (and a bit about precision and quantization)](https://www.youtube.com/watch?v=t509sv5MT0w)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29d974-0349-4fcb-b424-cc57ce0f5276",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## LoRA\n",
    "---\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://substackcdn.com/image/fetch/$s_!Fk3V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee7f7d37-3c0a-4f4a-9244-f73287af6211_1456x612.jpeg' alt='Visão Geral LoRa' style=\"width:800px;\"/>\n",
    "    <span style='display:block;'>Finetuning convencional x finetuning LoRA. Fonte: <a href=\"https://arxiv.org/html/2406.08464v2\" target=\"_blank\">Raschka</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4b908-1b18-4bb0-92e1-26f500f0a9b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe4e80-1d2c-43a5-baa7-873e7eeebf83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c174f8-941f-4ba6-a71a-22676c5e6b2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## LoRA\n",
    "---\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-5-1536x766.png\n",
    "' alt='Resultados LoRA' style=\"width:800px;\"/>\n",
    "    <span style='display:block;'>Resultados LoRA. Fonte: <a href=\"https://lightning.ai/pages/community/tutorial/lora-llm/\" target=\"_blank\">Lightning AI</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53889523",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Métricas de Avaliação I: Quantitativas e Técnicas\n",
    "\n",
    "A avaliação mede o quão bem o modelo ajustado fino generaliza e adere aos objetivos. Leia mais em [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/).\n",
    "\n",
    "*   **Cross-Entropy Loss (Perda de Entropia Cruzada):** A métrica fundamental monitorada durante o treinamento e a validação.\n",
    "    *   Quantifica a diferença entre a distribuição de probabilidade prevista pelo modelo e a distribuição real de *tokens*.\n",
    "*   **Métricas de PLN Tradicionais (para tarefas específicas):**\n",
    "    *   **BLEU:** Mede a proximidade entre traduções geradas e de referência (Tradução Automática, Sumarização).\n",
    "    *   **Acurácia/F1 Score:** Usado para tarefas de classificação e QA.\n",
    "*   **Avaliação de Codificação:**\n",
    "    *   [**HumanEval**](https://github.com/openai/human-eval): Consiste em 164 problemas de programação para avaliar a capacidade do modelo de gerar programas corretos a partir de *docstrings*.\n",
    "*   **Aderência à Instrução:**\n",
    "    *   [**IFEval (Instruction Following Evaluation)**](https://arxiv.org/pdf/2311.07911): Testa especificamente a capacidade de um modelo de seguir restrições explícitas, como contagem de palavras ou formatação de *output* necessária."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2f94f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Métricas de Avaliação II: Alinhamento e Pontuação\n",
    "---\n",
    "\n",
    "Para geração aberta, as métricas puramente automatizadas geralmente são insuficientes, exigindo avaliação centrada no ser humano.\n",
    "\n",
    "*   **LLM-como-Juiz (*LLM-as-a-Judge*):** Utiliza um LLM altamente capaz (e.g., Llama 3 8B) para avaliar a qualidade dos *outputs*.\n",
    "    *   O modelo Juiz recebe o *input*, o *output* correto e a resposta do modelo ajustado fino, fornecendo uma pontuação numérica (e.g., 0 a 100).\n",
    "    *   Isto é eficiente para avaliação em grande escala.\n",
    "*   **Benchmarking de Alinhamento:**\n",
    "    *   **MT-Bench:** Usa 80 questões multi-turno de alta qualidade para avaliar o alinhamento com a preferência humana, cobrindo tarefas como escrita, codificação e raciocínio.\n",
    "    *   **WildBench:** Curado a partir de interações reais do usuário, apresentando 1.024 instruções desafiadoras que exigem pensamento crítico.\n",
    "*   **Métricas de Segurança:** Avaliam respostas do LLM quanto a toxicidade, viés e aderência a diretrizes de segurança. Exemplos incluem Llama Guard 2/3 e ShieldGemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d676e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Armadilhas Comuns e Limitações em AFI\n",
    "---\n",
    "\n",
    "O AFI está sujeito a modos de falha específicos que podem minar a utilidade a longo prazo.\n",
    "\n",
    "*   **Esquecimento Catastrófico (*Catastrophic Forgetting*):** O ajuste fino em novas tarefas pode fazer com que o modelo perca o conhecimento pré-treinado.\n",
    "    *   *Mitigação:* Usar técnicas PEFT (LoRA/QLoRA) para congelar a maioria dos pesos base.\n",
    "*   **Alinhamento Superficial (*Superficial Alignment*):** O modelo aprende apenas padrões de superfície e estilos (e.g., formato de *output* ou tom) em vez de melhorar o raciocínio subjacente.\n",
    "    *   Isto levanta a preocupação de que os ganhos de desempenho dependam fortemente das tarefas representadas no dado de treinamento.\n",
    "*   **Dependência da Qualidade dos Dados:** O desempenho depende criticamente da qualidade, diversidade e cobertura de tarefas do conjunto de dados de instrução.\n",
    "    *   Dados mal selecionados (especialmente sintéticos) podem reforçar vieses ou deficiências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac64f88",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Resumo e Leitura Adicional\n",
    "---\n",
    "\n",
    "*   O AFI (SFT) é essencial para alinhar a previsão do próximo *token* dos LLMs com os objetivos do usuário.\n",
    "*   O AFI depende de pares de instrução-resposta de alta qualidade e diversificados, gerados manualmente (Flan, Dolly) ou sinteticamente (Self-Instruct, Evol-Instruct).\n",
    "*   O processo de treinamento usa uma perda de objetivo duplo e se beneficia de aprimoramentos arquiteturais (e.g., arquitetura de dois fluxos) e PEFT (LoRA/QLoRA).\n",
    "*   A avaliação requer métricas quantitativas (Entropia Cruzada, HumanEval) e técnicas qualitativas (LLM-como-Juiz, checagens de segurança).\n",
    "* **Leitura Adicional**:\n",
    "    * [Instruction Pretraining LLMs](https://magazine.sebastianraschka.com/p/instruction-pretraining-llms?utm_source=publication-search)\n",
    "    * [Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/pdf/2308.10792)\n",
    "    * [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities](https://arxiv.org/html/2408.13296v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ca83b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 1. Preparação de um Conjunto de Dados para Ajuste Fino Supervisionado por Instruções\n",
    "---\n",
    "\n",
    "Nesta seção, realizamos o download e a formatação do conjunto de dados de instrução destinado ao ajuste fino (finetuning) supervisionado de um modelo LLM pré‑treinado. O dataset contém 1 100 pares de *instruction–response* semelhantes aos apresentados anteriormente. Ele foi criado especificamente para este notebook; entretanto, existem outros conjuntos de dados de instruções disponíveis publicamente.\n",
    "\n",
    "O código a seguir implementa e executa uma função que baixa esse conjunto de dados — um arquivo relativamente pequeno, com apenas 204 KB, em formato JSON. O JSON (JavaScript Object Notation) espelha a estrutura dos dicionários Python, oferecendo uma representação simples e legível por humanos e também adequada para intercâmbio de dados entre máquinas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea6482",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(text_data)\n",
    "    else:                                     # Skip download if file was already downloaded\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"   # prepared by Sebastian Raschka (in Alpaca format)\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38685852",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb0226",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b9f699",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "**Ajuste fino por instruções** costuma ser chamado de *“supervised instruction finetuning”* porque envolve treinar um modelo em um conjunto de dados no qual os pares entrada‑saída são explicitamente fornecidos. Existem diferentes formas de formatar as entradas como input para o LLM; a figura abaixo ilustra dois formatos utilizados, respectivamente, nos modelos Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) e Phi‑3 (https://arxiv.org/abs/2404.14219):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff86e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"https://camo.githubusercontent.com/a0d3a2f932be145f5afd61d2abf54db358353ed2d30b4f5a54c508ac01a65b74/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f30342e776562703f32\" width=\"700px\">\n",
    "</center>\n",
    "\n",
    "> O estilo Alpaca (esquerda) emprega um formato estruturado com seções definidas para instrução, input e resposta;  \n",
    "> O estilo Phi‑3 (direita) utiliza um formato mais simples com tokens designados <|user|> e <|assistant|>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819f697",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Utilizamos a formatação de prompt no estilo Alpaca, que foi o template original para ajuste fino por instruções. A seguir, formatamos a entrada que será enviada ao LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a5578",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86dcf5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22753541",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5614d66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055ff99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d23ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 2. Organização dos Dados em Batches de Treino\n",
    "---\n",
    "\n",
    "Ao avançarmos na fase de implementação do processo de ajuste fino supervisionado por instruções, o próximo passo concentra-se na construção eficiente dos batches de treino. Isto requer definir um método que assegure que o modelo receba os dados formatados adequadamente durante o fine‑tuning.\n",
    "\n",
    "Em ajuste fino para classificação, os batches eram gerados automaticamente pela classe `DataLoader` do PyTorch, a qual utiliza uma função *collate* padrão para combinar listas de amostras em batches. A função *collate* é responsável por receber uma lista de dados individuais e consolidá‑la num único batch que pode ser processado eficientemente pelo modelo.\n",
    "\n",
    "Para ajuste fino supervisionado por instruções, o processo de batching envolve etapas adicionais: precisamos criar nossa própria função *collate* customizada, que será posteriormente inserida no `DataLoader`. Implementaremos essa função para atender às exigências específicas e à formatação do nosso conjunto de dados de instruções."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169459f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Dividimos a implementação do batching em cinco passos (conforme ilustrado na figura abaixo):\n",
    "\n",
    "</br>\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://camo.githubusercontent.com/305d903dda7544affa56b3f4e7c09adb2e2a1b65be56f40325422c1af0086d7e/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f30362e776562703f31' alt= '' style=\"height:550px;\"/>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42868ff4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Primeiro, implementamos a classe `InstructionDataset`, que pré‑tokeniza todas as entradas do dataset — de forma análoga à `SpamDataset` usada em classificação — passando do formato JSON ao texto e, daí, aos IDs dos tokens. Este processo de dois passos ocorre no construtor `__init__`.\n",
    "\n",
    "</br>\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://camo.githubusercontent.com/ffb7d4ced87d4044dd261909e8acff9155182556cc1ecfea0babb225a5e6e3a2/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f30372e776562703f31' alt= '' style=\"height:550px;\"/>\n",
    "    <br/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f25f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bb0f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Aqui, nós coletamos múltiplos exemplos de treinamento em um só batch para acelerar o treinamento. Isso requer padding em todas as entradas para gerar tamanhos similares e adicionar `<|endoftext|>` como token de padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70abf0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d958f98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Em ajuste fino por classificação, todos os exemplos eram padronizados para o mesmo comprimento globalmente. Aqui adotamos uma abordagem mais refinada: criamos uma função *collate* customizada que recebe cada batch individualmente e adiciona padding apenas até a maior sequência presente naquele batch (não em todo o dataset). Isto minimiza o padding desnecessário, pois batches diferentes podem ter comprimentos distintos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fda47f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "</br>\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://camo.githubusercontent.com/8ee26bf16d82ca4bcd948ba39de7cab6360980923bb66a2669c10d59af524fda/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f30382e776562703f31' alt= '' style=\"height:450px;\"/>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "A figura abaixo demonstra como os exemplos de treino são padronizados dentro de um batch usando o token ID 50256 para garantir comprimento uniforme. Cada batch pode ter tamanho diferente, como ilustrado nos dois primeiros batches da figura.\n",
    "\n",
    "Podemos implementar esse processo de padding com a função *collate* customizada que operará sobre cada batch (a *collate* cuidará das etapas 2.3 a 2.5; a implementação será feita em vários rascunhos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df8e66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(           # just taking care of step 2.3\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed049d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Esta função será posteriormente passada ao `DataLoader` para gerar batches adequados ao ajuste fino supervisionado por instruções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05838a29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a89e61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Esta figura ilustra o alinhamento entre os tokens de entrada e os tokens alvo utilizados no processo de ajuste fino por instruções de um LLM.\n",
    "\n",
    "</br>\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://camo.githubusercontent.com/526e35a8193b42968ab498d636171ec8568524a0b6d5726e9a0a5d4bdf73c5ef/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f31302e776562703f31' alt= '' style=\"height:650px;\"/>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dda11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030e58e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee692e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "No passo seguinte, atribuímos ao valor placeholder –100 a todos os tokens de padding, conforme ilustrado abaixo. Esse valor especial permite que excluamos esses tokens de padding do cálculo da perda de treinamento, garantindo que apenas dados significativos influenciem o aprendizado do modelo.\n",
    "\n",
    "</br>\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://camo.githubusercontent.com/190ff066e25553da764adce88f5ab471309504bc23a3af43c0e303186b41c93a/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f31322e776562703f32' alt= '' style=\"height:350px;\"/>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "Depois de criar a sequência alvo deslocando os IDs dos tokens uma posição para a direita e acrescentando um token de fim‑de‑texto, o passo 2.5 concentra‑se em substituir os tokens de padding de fim‑de‑texto por um valor placeholder (–100). Observe que mantemos um token de fim‑de‑texto (ID 50256) na lista alvo. Isso permite que o LLM aprenda quando gerar um token de fim‑de‑texto em resposta às instruções, o qual usamos como indicativo de que a resposta gerada está completa. Concretamente, isso significa substituir os IDs dos tokens correspondentes a 50256 por –100, conforme ilustrado acima.\n",
    "\n",
    "A figura mostra a substituição de todas as ocorrências do token de fim‑de‑texto, exceto a primeira (que usamos como padding), pelo valor placeholder –100, mantendo o token inicial de fim‑de‑texto em cada sequência alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fefd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "No trecho de código abaixo, modificamos nossa função de *collate* personalizada para substituir os tokens com ID 50256 por –100 nas listas de destino. Além disso, introduzimos o parâmetro `allowed_max_length` que permite limitar opcionalmente o tamanho das amostras. Essa alteração será útil caso você planeje trabalhar com seus próprios conjuntos de dados que excedam o limite de 1 024 tokens suportado pelo modelo GPT‑2. O código da função *collate* atualizada fica assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c529b4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,  # a default value that cross-entropy loss will ignore\n",
    "    allowed_max_length=None,  # truncate in case we have inputs exceeding the context length that the model supports\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index  # insert -100\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce475f41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9fe1e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "O primeiro tensor representa as inputs e o segundo representa os targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbda84b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Para testar, vamos ver como a troca para -100 ocorre. Vamos assumir uma pequena tarefa de classificação com 2 classes (0 e 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de596a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "z\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3eb1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Agora, se adicionarmos mais um exemplo de treino, a loss será influenciada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa202efa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5723296",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Vamos ver o que acontece se trocarmos o label da class pro um dos exemplos com -100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfbf73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287efcc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "O cálculo da loss ignorou o exemplo com label -100. Por padrão, o PyTorch possui a configuração cross_entropy(..., ignore_index=-100) que ignora exemplos correspondentes ao rótulo –100. Usando esse índice de ignorar –100, podemos descartar os tokens adicionais de fim‑de‑texto (padding) nos batches que usamos para padronizar os exemplos de treino a um mesmo comprimento. No entanto, desejamos manter um ID 50256 (fim‑de‑texto) nas metas porque isso ajuda o LLM a aprender a gerar tokens de fim‑de‑texto, os quais podemos usar como sinal de que uma resposta está completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760fe97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 3. Criando Data Loaders para um Dataset de Instruções\n",
    "\n",
    "---\n",
    "\n",
    "Até agora percorremos diversas etapas para implementar a classe `InstructionDataset` e a função `custom_collate_fn` para o dataset de instruções. Nesta seção, podemos colher os frutos do nosso trabalho simplesmente conectando ambos os objetos `InstructionDataset` e a função `custom_collate_fn` aos loaders de dados do PyTorch. Esses loaders irão embaralhar e organizar automaticamente os lotes (batches) para o processo de fine‑tuning de instruções no LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13173501",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8c5a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Um detalhe da função `custom_collate_fn` anterior é que agora movemos os dados diretamente para o dispositivo alvo (por exemplo, GPU), em vez de fazê‑lo dentro do loop principal de treinamento. Isso aumenta a eficiência porque pode ser executado como um processo em segundo plano quando usamos `custom_collate_fn` como parte do data loader.\n",
    "\n",
    "Usando a função `partial` da biblioteca padrão `functools` do Python, criamos uma nova função com o argumento `device` pré‑preenchido. O código abaixo inicializa a variável de dispositivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f98d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f19664",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn, # here we plug in our customized version\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed3af5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e673615",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Vamos verificar as dimensões de inputs e targets gerados pelo dataloader de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fba05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f024e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Na saída, podemos observar que o primeiro lote de entrada e alvo possui dimensões de 8×61, onde 8 representa o tamanho do lote (batch size) e 61 é o número de tokens em cada exemplo de treinamento neste lote. O segundo lote de entrada e alvo possui um número diferente de tokens, por exemplo, 76. Todos os lotes possuem um tamanho de lote de 8, mas um comprimento diferente, como esperado.\n",
    "\n",
    "Graças à nossa função `collate` personalizada, o data loader é capaz de criar lotes de comprimentos diferentes. Na próxima seção, carregaremos um LLM pré-treinado que poderemos então ajustar (finetune) com este data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bc8f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Vamos também verificar se as entradas contêm os tokens de preenchimento `<|endoftext|>` correspondentes ao ID do token 50256, imprimindo o conteúdo do primeiro exemplo de treinamento no último lote `inputs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0a8b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b16556",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Verificamos se os targets incluem -100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bcb6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b04ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 4. Carregando um LLM Pré-treinado\n",
    "---\n",
    "\n",
    "Acima, dedicamos muito tempo preparando o conjunto de dados para o ajuste fino (finetuning) por instrução, que é um aspecto fundamental do processo de ajuste supervisionado. Muitos outros aspectos são os mesmos que no pré-treinamento, permitindo-nos reutilizar grande parte do código de episódios anteriores. Especificamente, carregamos um modelo GPT pré-treinado usando o mesmo código de episódios anteriores, que serve como base para o treinamento subsequente. Este modelo pré-treinado, tendo já aprendido padrões e conhecimentos gerais da linguagem a partir de vastas quantidades de dados textuais, é então adaptado para seguir instruções através do processo de ajuste fino.\n",
    "\n",
    "No entanto, em vez de carregar o menor modelo com 124 milhões de parâmetros, carregamos a versão média com 355 milhões de parâmetros, pois o modelo de 124 milhões é pequeno demais para alcançar resultados qualitativamente razoáveis através do ajuste fino por instrução. Isso é feito usando o mesmo código dos dois episódios anteriores, exceto que agora especificamos \"gpt2-medium (355M)\" em vez de \"gpt2-small (124M)\", ou seja, estamos usando um modelo maior para obter melhores resultados.\n",
    "\n",
    "Observe que a execução do código fornecido abaixo iniciará o download da versão média do modelo GPT, que tem um requisito de armazenamento de aproximadamente 1,42 gigabytes. Este é mais ou menos três vezes maior do que o espaço de armazenamento necessário para o modelo pequeno:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68d4b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the same definitions we have used before:\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    # Send a GET request to download the file in streaming mode\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Get the total file size from headers, defaulting to 0 if not present\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Check if file exists and has the same size\n",
    "    if os.path.exists(destination):\n",
    "        file_size_local = os.path.getsize(destination)\n",
    "        if file_size == file_size_local:\n",
    "            print(f\"File already exists and is up-to-date: {destination}\")\n",
    "            return\n",
    "\n",
    "    # Define the block size for reading the file\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    # Initialize the progress bar with total file size\n",
    "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "        # Open the destination file in binary write mode\n",
    "        with open(destination, \"wb\") as file:\n",
    "            # Iterate over the file data in chunks\n",
    "            for chunk in response.iter_content(block_size):\n",
    "                progress_bar.update(len(chunk))  # Update progress bar\n",
    "                file.write(chunk)  # Write the chunk to the file\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f8b55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LLM_Definitions import GPTModel, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b21839",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Antes de começarmos o ajuste fino do modelo, vamos ver como ele se comporta em uma das tarefas de validação comparando sua saída com a resposta esperada. Isso nos dará um entendimento inicial de quão bem o modelo se comporta em uma tarefa de seguimento de instruções logo de cara, antes do ajuste fino, e nos ajudará a apreciar o impacto do ajuste fino mais tarde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a9d29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add4fd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Em seguida, geramos a resposta do modelo usando a função `generate` que já implementamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8f388",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LLM_Definitions import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d6b7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Note que a função `generate` retorna o texto de entrada e saída combinados. Esse comportamento foi conveniente em notebooks anteriores, já que os LLMs pré-treinados são projetados principalmente como modelos de completação de texto, onde a entrada e a saída são concatenadas para criar um texto coerente e legível. No entanto, ao avaliar o desempenho do modelo em uma tarefa específica, muitas vezes queremos nos concentrar apenas na resposta gerada pelo modelo.\n",
    "\n",
    "Para isolar o texto da resposta do modelo, precisamos subtrair o comprimento da instrução de entrada do início do `generated_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae84fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581d279",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Este trecho de código remove o texto da entrada do início do `generated_text`, deixando-nos apenas com a resposta gerada pelo modelo. A função `strip()` é então aplicada para remover quaisquer caracteres de espaço em branco à esquerda ou à direita.\n",
    "\n",
    "Como podemos ver, o modelo ainda não é capaz de seguir as instruções; ele cria uma seção \"Resposta\", mas simplesmente repete a frase original, bem como a instrução."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8bd88f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 5. Finetuning com Instruction Data\n",
    "---\n",
    "\n",
    "\n",
    "Agora, focamos no ajuste fino (finetuning) do modelo. Pegamos o modelo pré-treinado carregado na seção anterior e o treinamos ainda mais usando o conjunto de dados de instruções preparado anteriormente.\n",
    "\n",
    "Já fizemos todo o trabalho difícil quando implementamos o processamento do conjunto de dados de instruções no início deste notebook. Para o processo de ajuste fino em si, podemos reutilizar a função de cálculo da perda e as funções de treinamento implementadas durante o pré-treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf639467",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LLM_Definitions import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25144267",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Vamos calcular a perda inicial do conjunto de treinamento e validação antes de começar o treinamento (como em episódios anteriores, o objetivo é minimizar a perda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67672b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add33fd9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Note que o treinamento é um pouco mais caro do que em episódios anteriores, já que estamos usando um modelo maior (355 milhões de parâmetros em vez de 124 milhões). Os tempos de execução para vários dispositivos são mostrados abaixo para referência (executar este notebook em um dispositivo GPU compatível não requer alterações no código):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b6bd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| Model              | Device                | Runtime for 2 Epochs |\n",
    "|--------------------|-----------------------|----------------------|\n",
    "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |\n",
    "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |\n",
    "| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |\n",
    "| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |\n",
    "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |\n",
    "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |\n",
    "| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |\n",
    "| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8f9b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Com o modelo e os carregadores de dados preparados, podemos prosseguir com o treinamento do modelo. O código a seguir configura o processo de treinamento, incluindo a inicialização do otimizador, definindo o número de épocas e definindo a frequência de avaliação e o contexto inicial para avaliar as respostas LLM geradas durante o treinamento com base na primeira instrução do conjunto de validação (`val_data[0]`) que examinamos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884278a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd68d2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "A saída exibe o progresso do treinamento ao longo de duas épocas, onde a diminuição constante das perdas indica uma capacidade crescente de seguir instruções e gerar respostas apropriadas; portanto, o modelo treina bem. (Como o modelo demonstrou aprendizado eficaz nessas duas épocas, estender o treinamento para uma terceira época ou mais não é essencial e pode até ser contraproducente aqui, pois poderia levar a um aumento do sobreajuste.)\n",
    "\n",
    "Além disso, com base no texto da resposta impresso após cada época, podemos ver que o modelo segue corretamente a instrução de converter a frase de entrada `'The chef cooks the meal every day.'` para voz passiva `'The meal is cooked every day by the chef.'` (Formatar e avaliar as respostas adequadamente mais tarde).\n",
    "\n",
    "Finalmente, vamos dar uma olhada nas curvas de perda de treinamento e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c775f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LLM_Definitions import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2b1a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "A linha sólida representa a perda de treinamento, mostrando uma diminuição acentuada antes de se estabilizar, enquanto a linha tracejada representa a perda de validação, que segue um padrão semelhante.\n",
    "\n",
    "Como podemos ver, a perda diminui acentuadamente no início da primeira época, o que significa que o modelo começa a aprender rapidamente. Também podemos ver que um leve sobreajuste se instala por volta de 1 época de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc02b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 6. Extraindo e Salvando a Respostas\n",
    "---\n",
    "\n",
    "\n",
    "Depois de afinar o LLM na parte de treinamento do conjunto de instruções, agora prosseguimos para avaliar seu desempenho no conjunto de teste reservado. Para isso, primeiro extraímos as respostas geradas pelo modelo para cada entrada do conjunto de teste e as coletamos para análise manual, como ilustrado no panorama apresentado no início deste notebook.\n",
    "\n",
    "Começamos com o passo 7, a etapa de instrução de resposta, utilizando a função `generate`. A função `generate` retorna o texto combinado (entrada + saída), então usamos fatiamento (`slicing`) e o método `.replace()` sobre o conteúdo da variável `generated_text` para extrair apenas a resposta do modelo. Em seguida, imprimimos as respostas do modelo ao lado das respostas esperadas do conjunto de teste para os três primeiros itens, apresentando-as lado a lado para comparação:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859b99a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:            # Iterate over the first 3 test set samples\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(              # Use the generate function imported earlier\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33da7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Como podemos observar com base nas instruções do conjunto de teste, nas respostas fornecidas e nas respostas do modelo, o desempenho do modelo é relativamente bom. As respostas à primeira e última instrução estão claramente corretas. A segunda resposta está próxima; o modelo responde com “cumulus cloud” em vez de “cumulonimbus” (no entanto, observe que nuvens cumulus podem evoluir para nuvens cumulonimbus, que são capazes de produzir tempestades).\n",
    "\n",
    "O mais importante é perceber que a avaliação do modelo não é tão direta quanto no notebook anterior, onde bastava calcular a porcentagem de rótulos corretos de spam/não‑spam para obter a acurácia de classificação. Na prática, LLMs finetunados com instruções, como chatbots, são avaliados por múltiplas abordagens:\n",
    "\n",
    "- **Benchmarks de respostas curtas e escolha múltipla** (ex.: MMLU – “Measuring Massive Multitask Language Understanding”, <https://arxiv.org/abs/2009.03300>), que testam o conhecimento do modelo;\n",
    "\n",
    "- **Comparação de preferência humana** com outros LLMs, como a arena de chatbots LMSYS (<https://arena.lmsys.org>);\n",
    "\n",
    "- **Benchmarks conversacionais automatizados**, onde outro LLM (por exemplo, GPT‑4) avalia as respostas, como AlpacaEval (<https://tatsu-lab.github.io/alpaca_eval/>).\n",
    "\n",
    "Na prática, pode ser útil considerar os três tipos de métodos de avaliação: perguntas de múltipla escolha, avaliação humana e métricas automatizadas que medem o desempenho conversacional. Entretanto, visto que nosso foco principal é avaliar o desempenho em diálogos ao invés da mera capacidade de responder a questões de múltipla escolha, os métodos 2 (avaliação humana) e 3 (métricas automatizadas) podem ser mais relevantes. Considerando a escala da tarefa, implementaremos uma abordagem semelhante à do método 3, que envolve avaliar as respostas automaticamente usando outro LLM. Isso nos permitirá medir eficientemente a qualidade das respostas geradas sem necessidade de envolvimento humano extensivo, economizando tempo e recursos enquanto ainda obtemos indicadores significativos de desempenho.\n",
    "\n",
    "Na próxima seção, usaremos um procedimento parecido com o AlpacaEval e empregaremos outro LLM para avaliar as respostas do nosso modelo; porém, utilizaremos nosso próprio conjunto de teste ao invés de um dataset público. Isso possibilita uma avaliação mais direcionada e relevante do desempenho do modelo dentro do contexto dos casos de uso pretendidos representados em nosso conjunto de instruções. Para isso, anexamos as respostas geradas pelo modelo ao dicionário `test_data` e salvamos como arquivo `\"instruction-data-with-response.json\"` para fins de registro, permitindo que carreguemos e analisemos em sessões Python distintas, se necessário.\n",
    "\n",
    "O código a seguir usa o método `generate` da mesma forma que antes; porém, agora iteramos sobre todo o `test_set`. Além disso, ao invés de imprimir as respostas do modelo, adicionamos elas ao dicionário `test_set`, ou seja, salvamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1840eb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm   # use the progress bar library\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71101e89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808960fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Finalmente, salvamos o modelo como `gpt2-medium355M-sft.pth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5186e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove white spaces and parentheses from file name\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"  # sft = supervised finetuning\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n",
    "\n",
    "# Load the model via\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31421b0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 7. Avaliando um LLM-AFI\n",
    "---\n",
    "\n",
    "Anteriormente, avaliamos o desempenho de um modelo finetunado com instruções examinando suas respostas em 3 exemplos do conjunto de teste. Embora isso nos dê uma ideia aproximada de quão bem o modelo funciona, esse método não escala bem para grandes volumes de respostas. Portanto, nesta seção, automatizamos a avaliação das respostas do LLM finetunado usando outro LLM maior. Em particular, utilizamos um modelo Llama 3 de 8 bilhões de parâmetros finetunado com instruções pela Meta AI que pode ser executado localmente via **ollama** (<https://ollama.com>), e implementamos uma técnica para quantificar o desempenho do modelo finetunado pontuando as respostas geradas pelo teste.\n",
    "\n",
    "### Ollama\n",
    "\n",
    "O **ollama** é um aplicativo eficiente para gerenciar e interagir com grandes modelos de linguagem (LLMs) de forma prática, permitindo a execução desses modelos em laptops. Ele funciona como uma camada de abstração sobre a biblioteca open‑source *llama.cpp* (<https://github.com/ggerganov/llama.cpp>), que implementa LLMs em puro C/C++ para maximizar eficiência. Note que o Ollama é apenas uma ferramenta de inferência (geração de texto) e **não** suporta treinamento ou finetuning de LLMs.\n",
    "\n",
    "Antes de rodar o código abaixo, instale o Ollama visitando <https://ollama.com> e seguindo as instruções (por exemplo, clique em “Download” e baixe a aplicação do Ollama para seu sistema operacional).\n",
    "\n",
    "Em geral, antes de usar o Ollama via linha de comando, precisamos iniciar a aplicação ou executar `ollama serve` em um terminal separado.\n",
    "\n",
    "```html\n",
    "<br/>\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='https://camo.githubusercontent.com/01b2c5c8946336c8f500adbdf9eae0b942a5603a0762cbd7f6d65b7f38eba38d/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f32302e776562703f31' alt= '' style=\"height:450px;\"/>\n",
    "    <br/>\n",
    "</div>\n",
    "```\n",
    "\n",
    "A figura mostra duas opções para executar o Ollama. O painel esquerdo ilustra a inicialização usando `ollama serve`. O painel direito mostra uma segunda opção no macOS, executando a aplicação Ollama em segundo plano ao invés de usar o comando `ollama serve`.\n",
    "\n",
    "Com a aplicação Ollama ou `ollama serve` rodando em outro terminal, na linha de comando execute:\n",
    "\n",
    "```bash\n",
    "# 8B model\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "A saída fica assim:\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest\n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "> **Obs.:** `llama3` refere-se ao modelo Llama 3 de 8 bilhões de parâmetros finetunado com instruções. Usar o Ollama com o modelo `\"llama3\"` requer 16 GB de RAM; se seu computador não suportar, você pode tentar um modelo menor, como o phi‑3 de 3.8B, definindo `model = \"phi-3\"`, que exige apenas 8 GB de RAM. Alternativamente, pode usar o modelo maior Llama 3 de 70 bilhões de parâmetros (se sua máquina suportar), substituindo `llama3` por `llama3:70b`.\n",
    "\n",
    "Depois da descarga, aparecerá um prompt na linha de comando permitindo conversar com o modelo. Por exemplo:\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered\n",
    "stomach and eat plants that are high in fiber. In the wild, llamas\n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall\n",
    "grasses, wheat, oats, and barley.\n",
    "```\n",
    "\n",
    "> **Obs.:** A resposta pode variar pois o Ollama não é determinístico no momento da escrita.\n",
    "\n",
    "Você pode encerrar a sessão digitando `/bye`. Contudo, mantenha o comando `ollama serve` ou a aplicação Ollama em execução pelo restante desta etapa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f2006",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "O código a seguir verifica se a sessão do Ollama está funcionando corretamente antes de prosseguir com a avaliação das respostas geradas no conjunto de teste da seção anterior:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1491d07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab0891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Certifique‑se de que a saída ao executar o código anterior exiba `Ollama running: True`.  \n",
    "Se aparecer `False`, verifique se o comando `ollama serve` ou o aplicativo Ollama está em execução ativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51c1be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is optional; it allows you to restart the notebook\n",
    "# and only run the previous section without rerunning any of the previous code\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e78f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Uma forma alternativa de usar o comando `ollama run` para interagir com o modelo é por meio da sua API REST em Python, usando a função abaixo. Antes de executar as próximas células deste notebook, garanta que o Ollama ainda esteja rodando. A função `query_model` demonstra como utilizar essa API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b87b85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048  # context size\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "# now an example of how to use the query_llama function just implemented\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9c258",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Com a função `query_model` definida acima, podemos avaliar as respostas geradas pelo nosso modelo finetunado com um prompt que instrui o Llama 3 a pontuar as respostas do nosso modelo em uma escala de 0 a 100, usando a resposta do conjunto de teste como referência.\n",
    "\n",
    "Primeiro, aplicamos essa abordagem nos três primeiros exemplos do conjunto de teste que examinamos em uma seção anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affcf79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22ce72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Como podemos observar, o Llama 3 fornece uma avaliação razoável e também atribui pontos parciais quando o modelo não está totalmente correto, como no caso da resposta “cumulus cloud”. Note que o prompt original retorna avaliações muito detalhadas; podemos modificá‑lo para gerar apenas respostas inteiras entre 0 e 100 (onde 100 é a melhor pontuação). Essa modificação permite calcular uma média de pontuação do nosso modelo, oferecendo uma avaliação mais concisa e quantitativa.\n",
    "\n",
    "A função `generate_model_scores` abaixo usa um prompt alterado que instrui o modelo a “Responder apenas com o número inteiro”. A avaliação dos 110 itens do conjunto de teste leva cerca de 1 minuto em um laptop MacBook Air M3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c5461",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff49645",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Nosso modelo alcança uma pontuação média de aproximadamente **50**, que podemos usar como ponto de referência para comparar com outros modelos ou testar outras configurações de treinamento que possam melhorar o desempenho. Observe que o Ollama não é totalmente determinístico entre sistemas operacionais (até a data desta redação), então os números obtidos podem diferir ligeiramente dos mostrados acima.\n",
    "\n",
    "Para referência, as pontuações originais são:\n",
    "- Modelo base Llama 3 8B: **58,51**\n",
    "- Modelo Instruct Llama 3 8B: **82,65**\n",
    "\n",
    "Para melhorar ainda mais o desempenho do nosso modelo, podemos explorar várias estratégias, como:\n",
    "\n",
    "- Ajustar os hiperparâmetros durante o finetuning (taxa de aprendizado, tamanho do lote, número de épocas).\n",
    "- Aumentar o tamanho ou diversificar o conjunto de treinamento para cobrir uma gama maior de tópicos e estilos.\n",
    "- Experimentar diferentes prompts ou formatos de instrução que guiem as respostas do modelo de forma mais eficaz.\n",
    "- Considerar o uso de um modelo pré‑treinado maior, que pode ter maior capacidade de capturar padrões complexos e gerar respostas mais precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec347e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## 8. Conclusão\n",
    "---\n",
    "\n",
    "Cobrimos as etapas principais do ciclo de desenvolvimento de LLMs: implementar uma arquitetura de LLM, pré‑treinar um LLM e fine‑tune‑á-lo.\n",
    "\n",
    "Uma etapa opcional que às vezes é seguida após o fine‑tuning por instruções é o *preference finetuning* (fine‑tuning por preferência), que pode ser particularmente útil para personalizar um modelo de modo a se alinhar melhor com preferências específicas do usuário.\n",
    "\n",
    "Você pode estar interessado em usar LLMs diferentes e mais poderosos para aplicações do mundo real. Para isso, pode considerar ferramentas populares como **axolotl** ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)) ou **LitGPT** ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)).\n",
    "\n",
    "Os campos de IA e pesquisa em LLMs estão evoluindo num ritmo acelerado. Uma forma de acompanhar as últimas novidades é explorar artigos recentes no arXiv em https://arxiv.org/list/cs.LG/recent. Além disso, muitos pesquisadores e profissionais são muito ativos na divulgação e discussão das últimas inovações nas plataformas sociais como X (anteriormente Twitter) e Reddit. O subreddit **r/LocalLLaMA**, em particular, é um bom recurso para se conectar com a comunidade e ficar informado sobre as ferramentas e tendências mais recentes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": "80%",
   "overlay": "",
   "reveal_shortcuts": {
    "chalkboard": {
     "clear": "ctrl-k"
    },
    "main": {
     "toggleOverview": "tab"
    }
   },
   "scroll": true,
   "slideNumber": "c/t",
   "theme": "white",
   "transition": "slide",
   "width": "80%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
