{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b470226",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center>\n",
    "\n",
    "#\n",
    "\n",
    "# Tópicos em Ciência de Dados\n",
    "### Prof. Dr. Denis Mayr Lima Martins\n",
    "\n",
    "### Pontifícia Universidade Católica de Campinas\n",
    "\n",
    "<img src=\"https://www.puc-campinas.edu.br/wp-content/uploads/2022/06/logo-puc.png\" height=100px/>\n",
    "\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639006a9",
   "metadata": {},
   "source": [
    "# Mecanismos de Atenção\n",
    "\n",
    "Exploraremos o mecanismo de atenção que constitui a espinha dorsal dos LLMs. Estes mecanismos, e mais especificamente a self‑attention, permite que cada token de uma sequência se relacione contextualmente com os demais tokens, gerando representações altamente informativas. \n",
    "\n",
    "No contexto de Transformers, essa operação é realizada de forma paralela, escalada por múltiplas \"heads\" e otimizada pela normalização por $\\sqrt{d_k}$. A partir da teoria dos pesos de atenção, a arquitetura GPT e variantes modernas (BERT, T5, etc.) alcançam performance sem precedentes em tarefas de geração, compreensão e tradução automática. \n",
    "\n",
    "Este notebook guia o estudante desde os fundamentos matemáticos da atenção até a implementação prática em NumPy, culminando na aplicação do algoritmo em dados simulados para ilustrar como a atenção influencia a representação dos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9175f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eea0f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "944efddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    return np.random.rand(3)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ade32",
   "metadata": {},
   "source": [
    "## Tokenização e Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2787c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat hunts mouse\"\n",
    "words = sentence.split()\n",
    "embeddings = np.array([get_embeddings(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2e3b0",
   "metadata": {},
   "source": [
    "## Queries, Keys e Values\n",
    "\n",
    "Calcular  **projeção linear**: `Q`, `K` e `V` são obtidos multiplicando as embeddings pelos pesos correspondentes (`W_Q`, `W_K`, `W_V`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d8acbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73634365, 0.42647927, 0.85683087, 0.32620017],\n",
       "       [1.19358144, 0.83688612, 1.39485002, 0.64264091],\n",
       "       [1.11520082, 0.57175117, 1.27103597, 0.58396596],\n",
       "       [1.03917841, 0.67847982, 1.17557442, 0.75079734]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQ = np.random.random(embeddings.shape)\n",
    "Q = np.array(embeddings).dot(WQ.T)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee603ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75780912, 0.59683623, 0.78892439, 0.77407585],\n",
       "       [1.34918327, 1.02141999, 1.24144089, 0.85427868],\n",
       "       [1.16087731, 1.01643544, 1.35860721, 1.27956075],\n",
       "       [1.24943877, 1.08885856, 1.31923949, 0.73091859]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WK = np.random.random(embeddings.shape)\n",
    "K = np.array(embeddings).dot(WK.T)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "216989af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81080848, 0.2920271 , 0.68920257, 0.37484638],\n",
       "       [1.01372893, 0.81102414, 0.725672  , 0.60567394],\n",
       "       [1.22574627, 0.47592542, 1.1239937 , 0.72011269],\n",
       "       [0.74409112, 0.91022723, 0.58086821, 0.76392111]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WV = np.random.random(embeddings.shape)\n",
    "V = np.array(embeddings).dot(WV.T)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c02c3a",
   "metadata": {},
   "source": [
    "## Similaridade entre Q e V: Scaled Dot Product\n",
    "\n",
    " - Calcula a similaridade escalar entre cada par de tokens: $\\text{score} = \\frac{Q K^\\top}{\\sqrt{d_k}}$.  \n",
    "- O denominador normaliza o produto interno, evitando valores muito grandes que poderiam saturar a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b072898b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.74102466, 2.77144772, 2.86978282, 2.7531828 ],\n",
       "       [3.00187487, 4.7457906 , 4.95359371, 4.71241696],\n",
       "       [2.64113639, 4.16539406, 4.34981804, 4.11956372],\n",
       "       [2.70105363, 4.19585129, 4.45382426, 4.13679429]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores = Q.dot(K.T)\n",
    "similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "936b98cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = np.array(Q.shape[0]**0.5)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8bc94e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26782464, 1.05059507, 1.35240051, 1.13455545],\n",
       "       [2.22790493, 1.84235246, 2.36172036, 1.94668503],\n",
       "       [1.9108265 , 1.58868634, 2.043417  , 1.70244209],\n",
       "       [2.00283274, 1.6628199 , 2.12487558, 1.71850219]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_similarity_scores = Q.dot(K)/dim\n",
    "scaled_similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20987803",
   "metadata": {},
   "source": [
    "### Por que escalar os valores do produto escalar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fac108b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.09385167529024863),\n",
       " np.float64(0.06090284043555731),\n",
       " np.float64(0.8970449776197686))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.var(), K.var(), similarity_scores.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "975063e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.09385167529024863),\n",
       " np.float64(0.06090284043555731),\n",
       " np.float64(0.14133630407607445))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similaridade com variância em escala semelhante a Q e K\n",
    "Q.var(), K.var(), scaled_similarity_scores.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c0849",
   "metadata": {},
   "source": [
    "## Masked (Causal) Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99f67029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones(scaled_similarity_scores.shape))\n",
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a62bdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26782464,       -inf,       -inf,       -inf],\n",
       "       [2.22790493, 1.84235246,       -inf,       -inf],\n",
       "       [1.9108265 , 1.58868634, 2.043417  ,       -inf],\n",
       "       [2.00283274, 1.6628199 , 2.12487558, 1.71850219]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_scores = scaled_similarity_scores + mask\n",
    "masked_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203cd90",
   "metadata": {},
   "source": [
    "## Pesos de Atenção: Probabilidades com Softmax\n",
    "\n",
    "- Converte os scores em probabilidades (pesos de atenção) com $\\text{attention} = \\text{softmax}(\\text{score})$.  \n",
    "- Cada linha da matriz de pesos soma‑se a 1, interpretando‑se como a distribuição de \"atenção\" que o token dá aos demais tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0161faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13158609, 0.        , 0.        , 0.        ],\n",
       "       [0.34369052, 0.38290533, 0.        , 0.        ],\n",
       "       [0.25030074, 0.29711571, 0.47964661, 0.        ],\n",
       "       [0.27442264, 0.31997895, 0.52035339, 1.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = softmax(masked_scores)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20a395bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5809f1e",
   "metadata": {},
   "source": [
    "## Self-Attention: Combinação com Values\n",
    "\n",
    "| Representação Gráfica | Descrição |Fórmulação|\n",
    "|:----------------------|:----------|:---------|\n",
    "| <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/09/tour_3.png\" height=\"300px\"/>| A saída final para cada posição é uma <br> média ponderada dos vetores `V`, <br> usando os pesos calculados.|$\\text{Attention}(Q, K, V) = \\operatorname{Softmax}\\!\\left(\\frac{Q\\,K^{\\top}}{\\sqrt{d_k}}\\right)\\, V$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8190baaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10669112, 0.13339263, 0.16129116, 0.09791204],\n",
       "       [0.39048592, 0.65895449, 0.60351176, 0.60426792],\n",
       "       [0.62028548, 0.84277124, 0.98732989, 0.73530084],\n",
       "       [1.04942201, 1.52098066, 1.79364527, 1.56162686]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_output = attention_weights.dot(V.T)\n",
    "self_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb0ecd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    attention_weights, \n",
    "    xticklabels=words, \n",
    "    yticklabels=words, \n",
    "    square=True, \n",
    "    linewidth=1,\n",
    "    cmap=\"binary\",\n",
    "    )\n",
    "plt.title(\"Attention Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33889ef0",
   "metadata": {},
   "source": [
    "## Encapsulando Self-Attention em uma função completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e6a1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(embeddings):\n",
    "    \"\"\"\n",
    "    Aplica o mecanismo de *self‑attention* a um conjunto de embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Matriz de forma (n_tokens, d_model) contendo os vetores de entrada. \n",
    "        Cada linha corresponde ao embedding de um token da sequência.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy.ndarray\n",
    "        Representação contextualizada de cada token. A matriz tem a mesma\n",
    "        dimensionalidade que `embeddings` e reflete a soma ponderada dos\n",
    "        vetores de valor (V) segundo os pesos de atenção calculados.\n",
    "        \n",
    "    attention_weights : numpy.ndarray\n",
    "        Matriz de pesos de atenção (n_tokens × n_tokens). Cada linha representa \n",
    "        uma distribuição de probabilidade sobre os tokens da sequência para o token\n",
    "        correspondente.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1. **Geração das matrizes de projeção**  \n",
    "       As matrizes `W_Q`, `W_K` e `W_V` são inicializadas aleatoriamente com a mesma\n",
    "       forma que `embeddings`. Em uma implementação real, estas seriam parâmetros\n",
    "       aprendidos.\n",
    "\n",
    "    2. **Projeções**  \n",
    "       - `Q = embeddings @ W_Q.T`  \n",
    "       - `K = embeddings @ W_K.T`  \n",
    "       - `V = embeddings @ W_V.T`  \n",
    "\n",
    "    3. **Escalonamento do dot‑product**  \n",
    "       O escalar `dim` é calculado como a raiz quadrada da dimensão de Q/K\n",
    "       (i.e., `sqrt(d_model)`), evitando que os produtos internos cresçam demais\n",
    "       e mantenham a softmax numericamente estável.\n",
    "\n",
    "    4. **Máscara causal**  \n",
    "       A matriz triangular inferior (`np.tril`) impede que um token “veja”\n",
    "       tokens futuros, garantindo a causalidade em modelos autoregressivos.\n",
    "       Valores fora da diagonal são definidos como `-inf` para anular sua\n",
    "       contribuição após a softmax.\n",
    "\n",
    "    5. **Softmax**  \n",
    "       A função `softmax` (não mostrada aqui) transforma os scores mascarados\n",
    "       em probabilidades que somam 1 por linha, formando os pesos de atenção.\n",
    "\n",
    "    6. **Soma ponderada**  \n",
    "       O produto final `output = attention_weights @ V.T` combina os vetores\n",
    "       de valor segundo a distribuição aprendida, gerando as representações\n",
    "       contextuais.\n",
    "\n",
    "    7. **Limitações**  \n",
    "       - As matrizes de pesos são aleatórias e não treinadas; portanto o\n",
    "         resultado carece de significado prático sem treinamento supervisionado.\n",
    "       - O código assume que todas as operações se realizam em CPU; para GPU,\n",
    "         seria necessário adaptar a implementação usando `torch`.\n",
    "    \"\"\"\n",
    "    # Cria as matrizes de pesos para os embeddings\n",
    "    W_Q = np.random.random(embeddings.shape)\n",
    "    W_K = np.random.random(embeddings.shape)\n",
    "    W_V = np.random.random(embeddings.shape)\n",
    "\n",
    "    # Cria Q, K e V\n",
    "    Q = embeddings.dot(W_Q.T)\n",
    "    K = embeddings.dot(W_K.T)\n",
    "    V = embeddings.dot(W_V.T)\n",
    "\n",
    "    # Calcula a dimensão para escalar o dot product\n",
    "    dim = np.array(Q.shape[0]**0.5)\n",
    "\n",
    "    # Calcula o produto escalar dos embeddings\n",
    "    scores = Q.dot(K)/dim\n",
    "    \n",
    "    # Mascara os scores para evitar que tokens anteriores\n",
    "    #  tenham informação de tokens futuros\n",
    "    mask = np.tril(np.ones(scaled_similarity_scores.shape))\n",
    "    mask[mask == 0] = -np.inf\n",
    "    mask[mask == 1] = 0\n",
    "\n",
    "    masked_scores = scaled_similarity_scores + mask\n",
    "\n",
    "\n",
    "    # Aplica a softmax para obter os pesos de atenção\n",
    "    attention_weights = softmax(masked_scores)\n",
    "    \n",
    "    # Calcula a soma ponderada dos embeddings\n",
    "    output = np.dot(attention_weights, V.T)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4906b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attention_weights = self_attention(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b374b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03385798, 0.06795636, 0.05736033, 0.07451556],\n",
       "       [0.34351927, 0.62852834, 0.56964175, 0.65636921],\n",
       "       [0.44434504, 0.65118017, 0.75210792, 0.65922295],\n",
       "       [1.44358935, 2.07266063, 2.31967164, 1.89220321]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66845c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13158609, 0.        , 0.        , 0.        ],\n",
       "       [0.34369052, 0.38290533, 0.        , 0.        ],\n",
       "       [0.25030074, 0.29711571, 0.47964661, 0.        ],\n",
       "       [0.27442264, 0.31997895, 0.52035339, 1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798a51b",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "| Representação Gráfica | Descrição |\n",
    "|:--|:--:|\n",
    "|  Cada cabeça de atenção pode focar em <br> padrões diferentes (ex.: sintaxe, semântica, dependências a longo prazo).| <img src=\"https://www.jeremyjordan.me/content/images/2023/05/multi-head-attention.png\" height=\"200px\" /> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcf68d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída da MHA:\n",
      " [[0.0994691  0.12105612 0.14553897 0.07938693]\n",
      " [0.5373246  0.60720947 0.87887178 0.51157984]\n",
      " [0.63374816 0.92723863 1.03750033 0.88338746]\n",
      " [1.59325631 2.0461638  2.57622858 1.79480785]\n",
      " [0.07135296 0.06906638 0.1300595  0.07347102]\n",
      " [0.37417981 0.41672593 0.62458525 0.36774487]\n",
      " [0.62391651 0.96509893 0.9720056  0.8616684 ]\n",
      " [1.52339959 1.95082781 2.45765866 1.69994613]]\n",
      "Shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "def multi_head_attention(embeddings, num_heads=2):\n",
    "    \"\"\"\n",
    "    Aplica atenção multi‑cabeça (Multi‑Head Attention) em um conjunto de embeddings.\n",
    "\n",
    "    A função divide o cálculo da atenção em `num_heads` sub‑modelos independentes,\n",
    "    cada um produzindo uma representação distinta dos *queries*, *keys* e\n",
    "    *values*.  As saídas individuais são concatenadas ao longo do eixo das\n",
    "    características, gerando assim a projeção linear típica de uma camada\n",
    "    multi‑cabeça.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Tensor de entrada com forma ``(seq_len, d_model)`` onde\n",
    "        *seq_len* é o tamanho da sequência (ex.: número de tokens)\n",
    "        e *d_model* representa a dimensionalidade dos embeddings.\n",
    "        O array deve ser do tipo `float` ou equivalente.\n",
    "    num_heads : int, optional\n",
    "        Número de cabeças de atenção.  Cada cabeça executa uma chamada a\n",
    "        `self_attention(embeddings)` (uma função que deve retornar\n",
    "        `(output, attention_weights)`).  \n",
    "        O valor padrão é ``2``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Tensor resultante da concatenação das saídas de todas as cabeças.\n",
    "        A forma do retorno será ``(seq_len, num_heads * d_head)``,\n",
    "        onde `d_head = d_model // num_heads`.  Se `num_heads` não for divisor\n",
    "        exato de `d_model`, o último segmento pode ter dimensão menor.\n",
    "    \"\"\"\n",
    "    head_outputs = []\n",
    "    \n",
    "    for _ in range(num_heads):\n",
    "        output, _ = self_attention(embeddings)\n",
    "        head_outputs.append(output)\n",
    "    \n",
    "    # Concatena as saídas de todas as cabeças\n",
    "    return np.concatenate(head_outputs)\n",
    "\n",
    "# Calcula a atenção multi‑cabeça\n",
    "multi_head_output = multi_head_attention(embeddings)\n",
    "print(\"Saída da MHA:\\n\", multi_head_output)\n",
    "print(\"Shape:\", multi_head_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d73d2",
   "metadata": {},
   "source": [
    "## Camada linear de saída\n",
    "\n",
    "A camada linear que segue a concatenação das saídas da MHA serve fundamentalmente para **projetar** o vetor de dimensões ampliadas (produto do número de cabeças por dimensão de cada cabeça) de volta à dimensionalidade original do modelo (*d_model*). \n",
    "\n",
    "Essa projeção permite que as informações capturadas independentemente em cada cabeça sejam combinadas num único espaço sem aumentar a carga computacional nas camadas subsequentes. \n",
    "\n",
    "Além disso, ao aplicar pesos aprendidos e um viés, essa camada introduz capacidade de *mixer* entre os cabeças, facilitando a transmissão de dependências contextuais complexas, enquanto mantém o tamanho do modelo consistente para compatibilidade com as próximas etapas (normalização, feed‑forward, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc72adb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída Final:\n",
      " [[ 3.17695674  2.98664348  2.5831752   4.4084341 ]\n",
      " [ 8.88671905  7.90873057  7.94046735 11.94089099]\n",
      " [11.44750925 10.12335968 10.26517694 15.20395404]\n",
      " [23.87670969 20.82116613 21.93764431 31.59915826]\n",
      " [ 2.8737637   2.73318301  2.29232096  4.00703874]\n",
      " [ 6.81660573  6.12905012  5.99316222  9.20739846]\n",
      " [11.32358161 10.00487874 10.15262762 15.0336155 ]\n",
      " [22.84596507 19.93152506 20.97784673 30.24971987]]\n",
      "Shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "def feedforward_network(x):\n",
    "    \"\"\"\n",
    "    Aplica uma rede neural feed‑forward de duas camadas em um tensor de entrada.\n",
    "\n",
    "    A função implementa a arquitetura clássica “two‑layer perceptron” (MLP) com\n",
    "    ativação ReLU na camada oculta. Os pesos e vieses são gerados aleatoriamente\n",
    "    a cada chamada, o que facilita demonstrações didáticas mas não é adequado\n",
    "    para treinamento em produção.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Tensor de entrada de forma ``(batch_size, d_in)`` onde *d_in* corresponde à\n",
    "        dimensionalidade da representação de cada amostra. O array deve ser do tipo\n",
    "        `float` ou equivalente.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Saída da rede com forma ``(batch_size, d_out)``, onde  \n",
    "        ``d_out = d_in`` (o número de neurônios na camada de saída é igual ao\n",
    "        dimensionalidade de entrada). Cada elemento corresponde à projeção linear\n",
    "        do vetor de características após a aplicação da função ReLU.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Os pesos e vieses são inicializados com valores aleatórios em ``[0, 1)``\n",
    "      usando `np.random.rand`. Em cenários reais esses parâmetros seriam\n",
    "      aprendidos via back‑propagation.\n",
    "    - A camada oculta possui ``2 * d_in`` neurônios; o fator 2 pode ser ajustado\n",
    "      de acordo com a complexidade desejada.\n",
    "    \"\"\"\n",
    "    # Rede feed‑forward simples com uma camada oculta\n",
    "    # Pesos da camada oculta\n",
    "    W1 = np.random.rand(x.shape[1], x.shape[1] * 2)\n",
    "    # Viéses da camada oculta\n",
    "    b1 = np.random.rand(x.shape[1] * 2)              \n",
    "    # Pesos da camada de saída\n",
    "    W2 = np.random.rand(x.shape[1] * 2, x.shape[1])  \n",
    "    # Viéses da camada de saída\n",
    "    b2 = np.random.rand(x.shape[1])                  \n",
    "    \n",
    "    # Ativação ReLU\n",
    "    hidden_layer = np.maximum(0, np.dot(x, W1) + b1)  \n",
    "    output_layer = np.dot(hidden_layer, W2) + b2\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "# Passa as features de MHA pela rede (FC)\n",
    "final_output = feedforward_network(multi_head_output)\n",
    "print(\"Saída Final:\\n\", final_output)\n",
    "print(\"Shape:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766485b9",
   "metadata": {},
   "source": [
    "# Próximos Passos\n",
    "\n",
    "- Modifique o código acima para adicionar um tokenizer conforme visto nas aulas anteriores\n",
    "    - Tratamento de pontuação\n",
    "    - Tratamento de palavras desconhecidas\n",
    "    - Criação de Token IDs\n",
    "- Implemente via numpy positional enconding para os tokens.\n",
    "    - Use como base a implementação em [Machine Learning Mastery](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/).\n",
    "    - $P(k, 2i) = \\sin\\Big(\\frac{k}{n^{2i/d}}\\Big)$\n",
    "    - $P(k, 2i+1) = \\cos\\Big(\\frac{k}{n^{2i/d}}\\Big)$\n",
    "- Modifique o código acima para utilizar modelos de embeddings mais robustos (Word2Vec, BPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0c8c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-JkoWe6Ql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
