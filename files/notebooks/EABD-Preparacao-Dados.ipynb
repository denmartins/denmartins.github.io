{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec59a22",
   "metadata": {},
   "source": [
    "---\n",
    "# Estudos Avançados de Bancos de Dados\n",
    "### **Pontifícia Universidade Católica de Campinas**\n",
    "### **Prof. Dr. Denis Mayr Lima Martins**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0234707",
   "metadata": {},
   "source": [
    "# Aspectos de Preparação de Dados\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "\n",
    "- Entender o impacto da preparação de dados na análise.\n",
    "- Identificar problemas em dados (faltantes, repetidos, discrepantes, padronização).\n",
    "- Aplicar técnicas básicas para tratar esses problemas.\n",
    "- Utilizar Pandas para limpar e transformar dados.\n",
    "\n",
    "## Introdução\n",
    "\n",
    "A qualidade dos dados e a quantidade de informação útil que eles contêm são fatores-chave que determinam quão bem um algoritmo de aprendizado de máquina pode aprender. Portanto, é absolutamente crítico garantir que examinemos e pré-processemos um conjunto de dados antes de alimentá-lo a um algoritmo de aprendizado. Discutiremos as técnicas essenciais de pré-processamento de dados que nos ajudarão a construir bons modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44bd53",
   "metadata": {},
   "source": [
    "\n",
    "## Lidando com dados ausentes\n",
    "\n",
    "É comum em aplicações do mundo real que nossos exemplos de treinamento estejam faltando um ou mais valores por diversos motivos. Pode ter havido um erro no processo de coleta de dados, algumas medições podem não ser aplicáveis ou campos específicos simplesmente deixados em branco em uma pesquisa. Normalmente, vemos valores ausentes como espaços em branco na nossa tabela de dados ou como strings de marcador como NaN (que significa \"não é um número\") ou NULL (um indicador comumente usado de valores desconhecidos em bancos de dados relacionais). Infelizmente, a maioria das ferramentas computacionais não consegue lidar com esses valores ausentes ou produzirá resultados imprevisíveis se simplesmente os ignorarmos. Portanto, é crucial que cuidemos desses valores ausentes antes de prosseguirmos com análises adicionais.\n",
    "\n",
    "Vamos trabalhar com várias técnicas práticas para lidar com valores ausentes, removendo entradas do nosso conjunto de dados ou imputando valores ausentes de outros exemplos de treinamento e recursos.\n",
    "\n",
    "### Identificando valores ausentes em dados tabulares\n",
    "\n",
    "Antes de discutir várias técnicas para lidar com valores ausentes, vamos criar um dataframe simples a partir de um arquivo CSV (valores separados por vírgula) para ter uma melhor compreensão do problema. Além do Pandas, usamos o módulo IO, que fornece as principais facilidades do Python para lidar com vários tipos de I/O. Existem três tipos principais de I/O: I/O de texto, I/O binário e I/O bruto. O I/O de texto espera e produz objetos *str*; o módulo StringIO é um objeto semelhante a arquivo na memória que pode ser usado como entrada ou saída para a maioria das funções que esperariam um objeto de arquivo padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08085d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "csv_data = \\\n",
    "'''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "\n",
    "# If you are using Python 2.7, you need\n",
    "# to convert the string to unicode:\n",
    "\n",
    "if (sys.version_info < (3, 0)):\n",
    "    csv_data = unicode(csv_data)\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ad5a8",
   "metadata": {},
   "source": [
    "Lemos dados formatados em CSV para um pandas DataFrame usando a função `read_csv` e notamos que as duas células ausentes foram substituídas por NaN. A função `StringIO` no exemplo de código anterior foi usada apenas para fins ilustrativos. Ela nos permite ler a string atribuída a `csv_data` em um pandas DataFrame como se fosse um arquivo CSV regular em nosso disco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398a9d3",
   "metadata": {},
   "source": [
    "\n",
    "Podemos usar o método `isnull` para retornar um DataFrame com valores booleanos que indicam se uma célula contém um valor numérico (False) ou se os dados estão faltando (True). Usando o método `sum`, podemos então retornar o número de valores ausentes por coluna como segue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2716bde",
   "metadata": {},
   "source": [
    "\n",
    "### Eliminando exemplos de treinamento ou recursos com dados ausentes\n",
    "\n",
    "Uma das maneiras mais fáceis de lidar com dados ausentes é simplesmente remover os recursos (colunas) ou exemplos de treinamento (linhas) correspondentes do conjunto de dados por completo; linhas com valores ausentes podem ser facilmente removidas usando o método `dropna`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9020db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2590aa",
   "metadata": {},
   "source": [
    "\n",
    "Da mesma forma, podemos remover colunas que tenham pelo menos um NaN em qualquer linha definindo o argumento `axis` como 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bea0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35484ce",
   "metadata": {},
   "source": [
    "Podemos também definir um threshold para a remoção de linhas. No exemplo abaixo, as linhas com menos to que 4 valores preenchidos serão removidas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f52a2",
   "metadata": {},
   "source": [
    "Embora a remoção de dados ausentes pareça uma abordagem conveniente, ela também tem desvantagens: podemos remover amostras demais, tornando uma análise confiável impossível. \n",
    "\n",
    "Se removermos muitas colunas (ou as erradas), corremos o risco de perder informações valiosas. \n",
    "\n",
    "Como alternativa, por exemplo, em um conjunto de dados de filmes com uma coluna para duração, se a duração de alguns filmes for desconhecida, podemos substituí-la pela média da duração dos filmes para os quais ela é conhecida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a34aed",
   "metadata": {},
   "source": [
    "### Imputando valores ausentes\n",
    "\n",
    "Em vez da remoção de linhas ou da eliminação de colunas inteiras, podemos usar diferentes técnicas de interpolação para estimar os valores ausentes a partir dos outros exemplos de treinamento em nosso conjunto de dados. \n",
    "\n",
    "Uma das técnicas de interpolação mais comuns é a imputação por média, onde simplesmente substituímos o valor ausente pela média do valor da coluna de recursos inteira. Uma maneira conveniente de fazer isso é usando a classe `SimpleImputer` da scikit-learn, como mostrado no código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61defd",
   "metadata": {},
   "source": [
    "Substituímos cada valor NaN pela média correspondente, que é calculada separadamente para cada coluna. \n",
    "\n",
    "Outras opções para o parâmetro strategy são `median` ou `most_frequent`, onde o último substitui os valores ausentes pelos valores mais frequentes, respectivamente. Isso é útil para imputar valores de recursos categóricos, por exemplo, uma coluna de recursos que armazena uma codificação de nomes de cores, como vermelho, verde e azul.\n",
    "\n",
    "Alternativamente, uma maneira ainda mais conveniente de imputar valores ausentes é usando o método `fillna` do pandas e fornecendo um método de imputação como argumento. Por exemplo, podemos alcançar a mesma imputação por média diretamente no objeto DataFrame através do seguinte comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd77275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982f0fd",
   "metadata": {},
   "source": [
    "# Removendo Valores\n",
    "\n",
    "Utilizaremos técnicas estatísticas para identificar valores anormais (i.e. raros ou errados); esses dados poderão ser removidos ou selecionados para correção (veremos imputação mais adiante)\n",
    "\n",
    "## IQR (interquartil-range)\n",
    "\n",
    "Podemos usar as medidas de dispersão e distribuição para avaliar e encontrar amostras que estão muito distantes do esperado. Uma dar formas é o intervalo interquartil $IQR = (Q3 - Q1)$.\n",
    "\n",
    "O interquartil separa metade dos dados que estão na parte central, i.e. de 25% à 75%. Podemos pensar que valores que se afastam em uma proporção dessa amplitude são raros ou ruído.\n",
    "\n",
    "Considere o dataset abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "csv_data = \\\n",
    "'''0,24,21\n",
    "1,30,27\n",
    "2,26,24\n",
    "3,24,21\n",
    "4,27,22\n",
    "5,24,23\n",
    "6,25,23\n",
    "7,26,23\n",
    "8,23,21\n",
    "9,29,21\n",
    "10,23,21\n",
    "11,29,26\n",
    "12,27,24\n",
    "13,21,19\n",
    "14,22,25\n",
    "15,25,22\n",
    "16,21,20\n",
    "17,25,22\n",
    "18,20,18\n",
    "19,24,21\n",
    "20,18,17\n",
    "21,22,20\n",
    "22,17,16\n",
    "23,20,19\n",
    "0,29,28\n",
    "1,31,30\n",
    "2,31,30\n",
    "3,24,22\n",
    "4,25,20\n",
    "5,30,28\n",
    "6,23,21\n",
    "7,23,21\n",
    "8,25,24\n",
    "9,24,21\n",
    "10,23,22\n",
    "11,26,25\n",
    "12,23,20\n",
    "13,28,26\n",
    "14,26,10\n",
    "15,26,24\n",
    "16,50,21\n",
    "17,27,26\n",
    "18,23,22\n",
    "19,18,16\n",
    "20,22,20\n",
    "21,18,17\n",
    "22,19,17\n",
    "23,21,20\n",
    "0,32,31\n",
    "1,31,28\n",
    "2,26,42\n",
    "3,30,28\n",
    "4,13,29\n",
    "5,28,25\n",
    "6,28,25\n",
    "7,23,21\n",
    "8,25,22\n",
    "9,22,21\n",
    "10,22,21\n",
    "11,22,21\n",
    "12,25,21\n",
    "13,29,25\n",
    "14,23,21\n",
    "15,27,24\n",
    "16,19,17\n",
    "17,10,22\n",
    "18,18,17\n",
    "19,25,2\n",
    "20,18,15\n",
    "21,20,19\n",
    "22,20,17\n",
    "23,19,17\n",
    "0,27,25\n",
    "1,25,22\n",
    "2,25,22\n",
    "3,27,25\n",
    "4,24,55\n",
    "5,29,26\n",
    "6,80,24\n",
    "7,27,24\n",
    "8,25,12\n",
    "9,26,7\n",
    "10,23,20\n",
    "11,48,24\n",
    "12,21,18\n",
    "13,28,27\n",
    "14,28,24\n",
    "15,24,22\n",
    "16,22,19\n",
    "17,22,19\n",
    "18,27,25\n",
    "19,23,20\n",
    "20,25,24\n",
    "21,17,15\n",
    "22,21,20\n",
    "23,24,23\n",
    "0,30,26\n",
    "1,24,20\n",
    "2,31,29\n",
    "3,35,27\n",
    "4,30,26\n",
    "5,30,26\n",
    "6,30,26\n",
    "7,28,24\n",
    "8,26,24\n",
    "9,26,22\n",
    "10,25,22\n",
    "11,28,25\n",
    "12,26,18\n",
    "13,29,27\n",
    "14,20,19\n",
    "15,20,17\n",
    "16,20,17\n",
    "17,21,20\n",
    "18,24,22\n",
    "19,24,22\n",
    "20,19,18\n",
    "21,20,23\n",
    "22,21,40\n",
    "23,25,24\n",
    "0,40,25\n",
    "1,29,26\n",
    "2,28,26\n",
    "3,31,23\n",
    "4,24,23\n",
    "5,30,26\n",
    "6,23,19\n",
    "7,26,23\n",
    "8,27,24\n",
    "9,23,21\n",
    "10,28,25\n",
    "11,29,26\n",
    "12,29,27\n",
    "13,21,19\n",
    "14,23,21\n",
    "15,28,25\n",
    "16,19,17\n",
    "17,22,18\n",
    "18,22,16\n",
    "19,23,21\n",
    "20,21,18\n",
    "21,26,25\n",
    "22,23,20\n",
    "23,19,18\n",
    "0,30,29\n",
    "1,33,31\n",
    "2,29,26\n",
    "3,25,21\n",
    "4,29,27\n",
    "5,24,21\n",
    "6,29,28\n",
    "7,23,22\n",
    "8,27,25\n",
    "9,30,27\n",
    "10,23,21\n",
    "11,29,28\n",
    "12,25,24\n",
    "13,27,25\n",
    "14,27,26\n",
    "15,29,28\n",
    "16,27,26\n",
    "17,24,21\n",
    "18,25,22\n",
    "19,24,23\n",
    "20,27,26\n",
    "21,21,20\n",
    "22,19,16\n",
    "23,22,21\n",
    "0,24,22\n",
    "1,28,27\n",
    "2,31,28\n",
    "3,26,24\n",
    "4,30,27\n",
    "5,31,28\n",
    "6,31,28\n",
    "7,30,29\n",
    "8,23,19\n",
    "9,26,23\n",
    "10,29,26\n",
    "11,29,26\n",
    "12,28,26\n",
    "13,22,18\n",
    "14,22,19\n",
    "15,25,23\n",
    "16,22,20\n",
    "17,27,23\n",
    "18,22,21\n",
    "19,18,15\n",
    "20,24,22\n",
    "21,21,18\n",
    "22,20,18\n",
    "23,26,25\n",
    "0,32,31\n",
    "1,31,30\n",
    "2,32,29\n",
    "3,30,27\n",
    "4,27,24\n",
    "5,25,23\n",
    "6,29,28\n",
    "7,24,22\n",
    "8,27,25\n",
    "9,30,29\n",
    "10,24,23\n",
    "11,22,18\n",
    "12,30,28\n",
    "13,26,25\n",
    "14,21,19\n",
    "15,27,25\n",
    "16,21,18\n",
    "17,25,21\n",
    "18,21,19\n",
    "19,26,25\n",
    "20,26,25\n",
    "21,23,22\n",
    "22,21,17\n",
    "23,26,23'''\n",
    "\n",
    "# If you are using Python 2.7, you need\n",
    "# to convert the string to unicode:\n",
    "\n",
    "if (sys.version_info < (3, 0)):\n",
    "    csv_data = unicode(csv_data)\n",
    "\n",
    "vendas = pd.read_csv(StringIO(csv_data))\n",
    "vendas.columns=['hora','prod1','prod2']\n",
    "vendas[['prod1','prod2']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df430ded",
   "metadata": {},
   "source": [
    "Por exemplo, vemos que para o produto 1, temos entre 10 e 80 vendas por hora; mas em 50% das horas amostradas, temos entre 22 e 28 vendas. É intuitivo pensar que se na metade das horas as vendas oscilam em 6 pontos (nosso IQR), valores que se distanciem em 6 pontos do Q1 e do Q3 sejam bem mais raros. Em aplicações reais tendemos a ser ainda mais permissivos na nossa margem, **multiplicamos o IQR por 1.5** e adicionamos ou subtraímos de Q3 e Q1 (respectivamente) para definir a nossa região de dados 'comuns'. Esse valor é tão usado que é pradrão em um plot estatístico, o **boxplot**\n",
    "\n",
    "'> (Q3 + IQR*1.5) - Outlier\n",
    "\n",
    "'< (Q1 - IQR*1.5) - Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendas.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando IQR e margem 'válida'\n",
    "IQR = vendas[['prod1','prod2']].quantile(0.75) - vendas[['prod1','prod2']].quantile(0.25)\n",
    "margemMin = vendas[['prod1','prod2']].quantile(0.25) - IQR*1.5\n",
    "margemMax = vendas[['prod1','prod2']].quantile(0.75) + IQR*1.5\n",
    "\n",
    "#Filtrando valores anormais/outliers\n",
    "idx=[]\n",
    "for col in ['prod1','prod2']:\n",
    "    filtered = vendas[col][(vendas[col] < margemMin[col]) | (vendas[col] > margemMax[col])]\n",
    "    idx.extend(filtered.index)\n",
    "vendas.iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee9643",
   "metadata": {},
   "source": [
    "#### Alta distância para a média em termos de desvios padrões"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b7f16",
   "metadata": {},
   "source": [
    "Ainda sob essa perspectiva de usar alguma medida de amplitude para medir valores anormais. Podemos utilizar o desvio padrão. Sabemos que em amostras simétricas temos 68% das amostras dentro de 1 desvio padrão da média, e quando aumentamos essa margem para 3 desvios padrões temos cerca de 99.7%. Iremos filtrar fora todas as amostras que estão a 3 desvios padrões da média, para mais ou para menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando IQR e margem 'válida'\n",
    "deviation = vendas[['prod1','prod2']].std()\n",
    "margemMin = vendas[['prod1','prod2']].mean() - 3*deviation\n",
    "margemMax = vendas[['prod1','prod2']].mean() + 3*deviation\n",
    "\n",
    "#Filtrando valores anormais/outliers\n",
    "idx=[]\n",
    "for col in ['prod1','prod2']:\n",
    "    filtered = vendas[col][(vendas[col] < margemMin[col]) | (vendas[col] > margemMax[col])]\n",
    "    idx.extend(filtered.index)\n",
    "vendas.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vendas.info() #nossa base original possui 216 amostras\n",
    "vendas.iloc[idx].info() #dessas 216, 8 estão muito fora da média ~ 3,7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e56bd1",
   "metadata": {},
   "source": [
    "**Dicussões e reflexões:**\n",
    "- Notamos que cada abordagem se comporta de uma maneira diferente. A IQR se preocupa com a amplitude, independente de assimetria ou desvio padrão; enquanto que a baseada em 3 desvios padrões tem uma interpretação muito boa para distribuições normais\n",
    "- Sempre serão usados esse limiares (i.e. 3 desvios e 1.5 vezes o IQR)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8462ea",
   "metadata": {},
   "source": [
    "## Trabalhando com Dados Categóricos\n",
    "\n",
    "Até agora, trabalhamos apenas com valores numéricos. No entanto, é comum que conjuntos de dados do mundo real contenham uma ou mais colunas de valores categóricos.\n",
    "\n",
    "Exemplos de dados categóricos ou variáveis categóricas:\n",
    "\n",
    "*   Informações demográficas de uma população: gênero, status da doença.\n",
    "*   O tipo sanguíneo de uma pessoa: A, B, AB ou O.\n",
    "\n",
    "Quando falamos sobre dados categóricos, precisamos distinguir entre **ordinais** e **nominais**. Dados ordinais podem ser entendidos como valores categóricos que podem ser ordenados. Por exemplo, o tamanho da camiseta seria um dado ordinal, porque podemos definir uma ordem: XL > L > M. Em contraste, os dados nominais não implicam nenhuma ordem e, continuando com o exemplo anterior, poderíamos pensar na cor da camiseta como um recurso nominal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([['green', 'M', 10.1, 'class2'],\n",
    "                   ['red', 'L', 13.5, 'class1'],\n",
    "                   ['blue', 'XL', 15.3, 'class2']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a574d3",
   "metadata": {},
   "source": [
    "\n",
    "Como podemos ver, o novo DataFrame contém um dado nominal (cor), um dado ordinal (tamanho) e uma coluna numérica (preço). As etiquetas (labels) de classe (assumindo que criamos um conjunto de dados para uma tarefa de aprendizado supervisionado) são armazenadas na última coluna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fcacd4",
   "metadata": {},
   "source": [
    "### Mapeamento de dados ordinais\n",
    "\n",
    "Para garantir que um modelo interprete corretamente os dados ordinais, precisamos converter os valores categóricos da string em inteiros. \n",
    "\n",
    "No seguinte exemplo simples, vamos supor que sabemos a diferença numérica entre os recursos, por exemplo, XL = L + 1 = M + 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mapping = {'XL': 3,\n",
    "                'L': 2,\n",
    "                'M': 1}\n",
    "\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be69e57",
   "metadata": {},
   "source": [
    "### Codificação das etiquetas de classe\n",
    "\n",
    "Para codificar as etiquetas (labels) de classe, podemos usar uma abordagem semelhante ao mapeamento de recursos ordinais discutido anteriormente. Precisamos lembrar que as etiquetas de classe não são ordinais e não importa qual número inteiro atribuímos a uma determinada etiqueta de string. Assim, podemos simplesmente enumerar as etiquetas de classe, começando em 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ec787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# cria um dicionário de mapeamento\n",
    "# para converter as etiquetas de classe de strings para inteiros\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}\n",
    "# para converter as etiquetas de classe de strings para inteiros\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af96599",
   "metadata": {},
   "source": [
    "### One-Hot Encoding: Transformando Categorias em Números\n",
    "\n",
    "Em muitos problemas de aprendizado de máquina, os dados precisam ser preparados adequadamente. Quando você tem variáveis categóricas (como cores, tipos de produtos ou regiões), o algoritmo precisa entender essas categorias como números. O one-hot encoding é uma técnica que resolve isso.\n",
    "\n",
    "Basicamente, ele transforma cada categoria em uma nova coluna binária (0 ou 1). Uma linha representa um exemplo e, para cada categoria, a coluna correspondente terá um '1' se o exemplo pertencer àquela categoria e '0' caso contrário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding via pandas\n",
    "pd.get_dummies(df[['price', 'color', 'size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a797ea8",
   "metadata": {},
   "source": [
    "# Feature Scaling: Normalizando Seus Dados para o Sucesso\n",
    "\n",
    "Em muitos algoritmos de aprendizado de máquina, as características (features) dos seus dados podem ter escalas muito diferentes. Por exemplo, a idade de uma pessoa pode variar de 20 a 80 anos, enquanto o número de filhos pode variar de 0 a 10. Essa disparidade de escalas pode prejudicar o desempenho de alguns algoritmos, especialmente aqueles baseados em distância (como k-NN ou regressão linear).\n",
    "\n",
    "Considere o dataset abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Criando o DataFrame\n",
    "data = {'Idade': [25, 30, 45, 60, 38],\n",
    "        'Renda': [50000, 75000, 120000, 90000, 62000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9579a6",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Scaling** é uma técnica que visa transformar todas as características para terem uma escala semelhante. Existem duas abordagens principais:\n",
    "\n",
    "*   **Min-Max Scaling:** Escala os valores para um intervalo entre 0 e 1.\n",
    "*   **Standardization (Z-Score):** Transforma os valores para ter média 0 e desvio padrão 1.\n",
    "\n",
    "Ao aplicar o feature scaling, você garante que nenhuma característica domine as outras durante o treinamento do modelo, melhorando a convergência e precisão dos resultados.\n",
    "\n",
    "Em resumo, o Feature Scaling é um passo importante na preparação de dados para aprendizado de máquina, garantindo que todos os recursos contribuam igualmente para o processo de modelagem.\n",
    "\n",
    "Para \"escalar\" nossos dados, podemos simplesmente aplicar a escala Min-Max a cada coluna de recurso, onde o novo valor $x_{norm}^{(i)}$ do exemplo $x^{(i)}$ pode ser calculado da seguinte forma:\n",
    "\n",
    "$$x_{norm}^{(i)} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Aqui, $x_{min}$ é o menor valor na coluna de recurso e $x_{max}$ é o maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "df_scaled = mms.fit_transform(df)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccf3ed",
   "metadata": {},
   "source": [
    "Já o Z-Score transforma os valores de uma característica, centralizando-os em torno de zero e escalando-os com base no desvio padrão. A fórmula é:\n",
    "\n",
    "$$x_{scaled}^{(i)} = \\frac{x^{(i)} - μ}{σ}$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "*   $x_{scaled}^{(i)}$ é o valor normalizado da característica $i$.\n",
    "*   $x^{(i)}$ é o valor original da característica $i$.\n",
    "*   μ (mu) é a média (média aritmética) de todos os valores da característica.\n",
    "*   σ (sigma) é o desvio padrão de todos os valores da característica.\n",
    "\n",
    "Em resumo, subtraímos a média da característica e dividimos pelo seu desvio padrão, resultando em um valor que indica quantos desvios padrão um ponto está da média. Isso ajuda a remover o impacto de diferentes escalas e permite que algoritmos mais sensíveis à escala funcionem corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13182303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "df_scaled = std.fit_transform(df)\n",
    "df_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
